{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 3: LSTM and GRU vs. Multiplicative Variations\n",
        "\n",
        "### Maxim Ryabinov (U02204083)\n",
        "### CAP4641: Natural Language Processing \n",
        "### Instructor: Dr. Ankur Mali \n",
        "### University of South Florida (Spring 2025)\n",
        "\n",
        "---\n",
        "\n",
        "# Description\n",
        "\n",
        "In this assignment, I implemented Standard LSTM RNN, Standard GRU RNN, Multiplicative LSTM RNN, and Multiplicative GRU RNN. My choice for the machine learning library used in this notebook is TensorFlow.\n",
        "\n",
        "Below, you will find an implementation for each recurrent neural network architecture, all following a set of model equations that each of the architectures are based off of.\n",
        "\n",
        "Lastly, in order to show robustness and highlight the differences in gating mechanisms between the architectures, the Copy Task is carried out. Each model is ran through this task using varying lengths of sequences that contain a randomly generated set of characters (sequences lengths: `{100, 200, 500, 1000}`).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-16 15:48:36.544347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742154516.561974   31890 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742154516.567053   31890 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-16 15:48:36.584901: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "seed = 123\n",
        "\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# # Make TensorFlow deterministic\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Force TensorFlow to use deterministic ops\n",
        "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # Ensure cuDNN is deterministic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Defining each RNN Architecture\n",
        "\n",
        "### Standard LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        i_t = tf.sigmoid(tf.matmul(x_t, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_t, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_t, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_t, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size, epochs, batch_size, learning_rate, optimizer, loss_fn):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = StandardLSTMCell(input_size, hidden_size)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state = None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "            c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h, c = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), (h, c)  # [batch_size, seq_length, input_size] and h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights and bias\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights and bias\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights and bias\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights and bias\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Multiplicative extension weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        # Multiplicative Extension\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        i_t = tf.sigmoid(tf.matmul(x_cap, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_cap, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_cap, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_cap, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "class MultiplicativeLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size, epochs, batch_size, learning_rate, optimizer, loss_fn):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = MultiplicativeLSTMCell(input_size, hidden_size)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "\n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "            c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h, c = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), (h, c)  # [batch_size, seq_length, input_size] and h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard GRU RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardGRUCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate weights\n",
        "        self.W_z = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_z = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_z = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Reset gate weights\n",
        "        self.W_r = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_r = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_r = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Candidate hidden state weights\n",
        "        self.W_h = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_h = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_h = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        # Update gate\n",
        "        z_t = tf.sigmoid(tf.matmul(x_t, self.W_z) + tf.matmul(h_prev, self.U_z) + self.b_z)\n",
        "        \n",
        "        # Reset gate\n",
        "        r_t = tf.sigmoid(tf.matmul(x_t, self.W_r) + tf.matmul(h_prev, self.U_r) + self.b_r)\n",
        "        \n",
        "        # Candidate hidden state\n",
        "        h_hat = tf.tanh(tf.matmul(x_t, self.W_h) + tf.matmul(r_t * h_prev, self.U_h) + self.b_h)\n",
        "        \n",
        "        # New hidden state\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_hat\n",
        "        \n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardGRU(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size, epochs, batch_size, learning_rate, optimizer, loss_fn):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = StandardGRUCell(input_size, hidden_size)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.gru_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), h  # [batch_size, seq_length, input_size] and h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative GRU RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeGRUCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate weights\n",
        "        self.W_z = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_z = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_z = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Reset gate weights\n",
        "        self.W_r = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_r = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_r = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Candidate hidden state weights\n",
        "        self.W_h = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_h = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_h = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Multiplicative extension weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        # Memory matrix introduction\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        # Update gate\n",
        "        z_t = tf.sigmoid(tf.matmul(x_cap, self.W_z) + tf.matmul(h_prev, self.U_z) + self.b_z)\n",
        "        \n",
        "        # Reset gate\n",
        "        r_t = tf.sigmoid(tf.matmul(x_cap, self.W_r) + tf.matmul(h_prev, self.U_r) + self.b_r)\n",
        "        \n",
        "        # Candidate hidden state\n",
        "        h_hat = tf.tanh(tf.matmul(x_cap, self.W_h) + tf.matmul(r_t * h_prev, self.U_h) + self.b_h)\n",
        "        \n",
        "        # New hidden state\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_hat\n",
        "        \n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class MultiplicativeGRU(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size, epochs, batch_size, learning_rate, optimizer, loss_fn):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = MultiplicativeGRUCell(input_size, hidden_size)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.gru_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), h  # [batch_size, seq_length, input_size] and h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Running the Copy Task\n",
        "### Train, Test, and Validation Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset_splits(sequence_count, input_size, training_length, sequence_length, delimiter, total_delimiters):\n",
        "    X_train = np.random.randint(0, 10, size=(sequence_count, training_length, input_size)).astype(np.float32)\n",
        "    X_val = np.random.randint(0, 10, size=(sequence_count, training_length, input_size)).astype(np.float32)\n",
        "    X_test = np.random.randint(0, 10, size=(sequence_count, sequence_length, input_size)).astype(np.float32)\n",
        "    delimiters = np.full((sequence_count, total_delimiters, input_size), delimiter, dtype=np.float32)\n",
        "    \n",
        "    X_train = np.concatenate([X_train, delimiters], axis=1)\n",
        "    X_val = np.concatenate([X_val, delimiters], axis=1)\n",
        "    X_test = np.concatenate([X_test, delimiters], axis=1)\n",
        "    \n",
        "    Y_train = X_train.copy()\n",
        "    Y_val = X_val.copy()\n",
        "    Y_test = X_test.copy()\n",
        "    \n",
        "    return X_train, X_val, X_test, Y_train, Y_val, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Validating the Model (Training Loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, X_train, Y_train, X_val, Y_val):\n",
        "    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y_train = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "    X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "    Y_val = tf.convert_to_tensor(Y_val, dtype=tf.float32)\n",
        "    \n",
        "    # Used for plotting the graphs \n",
        "    train_loss_data = []\n",
        "    val_loss_data = []\n",
        "    train_accuracy_data = []\n",
        "    val_accuracy_data = []\n",
        "    \n",
        "    for epoch in range(model.epochs):\n",
        "        # Shuffle training data at the start of each epoch.\n",
        "        indices = tf.range(start=X_train.shape[0])\n",
        "        indices = tf.random.shuffle(indices)\n",
        "        X_train = tf.gather(X_train, indices)\n",
        "        Y_train = tf.gather(Y_train, indices)\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        num_batches = int(np.ceil(X_train.shape[0] / model.batch_size))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start = i * model.batch_size\n",
        "            end = min((i+1) * model.batch_size, X_train.shape[0])\n",
        "            X_batch = X_train[start:end]\n",
        "            Y_batch = Y_train[start:end]\n",
        "            \n",
        "            # Convert one-hot encoded labels to integer class labels\n",
        "            Y_batch_labels = tf.argmax(Y_batch, axis=-1)  # Shape will be (batch_size, seq_length)\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "                output, _ = model(X_batch)\n",
        "                batch_loss = model.loss_fn(Y_batch_labels, output)\n",
        "            gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            \n",
        "            epoch_loss += batch_loss.numpy()\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        \n",
        "        # Calculate training accuracy\n",
        "        train_preds = np.argmax(output.numpy(), axis=-1).flatten()\n",
        "        true_train = Y_batch_labels.numpy().flatten()\n",
        "        train_accuracy = np.mean(train_preds == true_train)\n",
        "        \n",
        "        # Calculate validation loss and accuracy\n",
        "        val_output, _ = model(X_val)\n",
        "        val_loss = model.loss_fn(tf.argmax(Y_val, axis=-1), val_output).numpy()\n",
        "        \n",
        "        val_preds = np.argmax(val_output.numpy(), axis=-1)\n",
        "        true_val = tf.argmax(Y_val, axis=-1).numpy()\n",
        "        val_accuracy = np.mean(val_preds == true_val)\n",
        "        \n",
        "        # Store metrics for plotting\n",
        "        train_loss_data.append(epoch_loss)\n",
        "        val_loss_data.append(val_loss)\n",
        "        train_accuracy_data.append(train_accuracy)\n",
        "        val_accuracy_data.append(val_accuracy)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "              f\"Training Accuracy: {train_accuracy:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
        "        \n",
        "    # # After training loop\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "    # # Training data\n",
        "    # plt.plot(train_loss_data, label=\"Training Loss\", marker=\".\", color=\"#1f77b4\")\n",
        "    # plt.plot(train_accuracy_data, label=\"Training Accuracy\", marker=\".\", color=\"#66c2ff\")\n",
        "    \n",
        "    # # Validation data\n",
        "    # plt.plot(val_loss_data, label=\"Validation Loss\", marker=\".\", color=\"#d62728\")\n",
        "    # plt.plot(val_accuracy_data, label=\"Validation Accuracy\", marker=\".\", color=\"#ff7f7f\")\n",
        "\n",
        "    # plt.xticks(range(len(train_loss_data))) # To display all epochs on x axis.\n",
        "\n",
        "    # plt.xlabel(\"Epoch\")\n",
        "    # plt.ylabel(\"Loss and Accuracy\")\n",
        "    # plt.title(\"Training and Validation Loss/Accuracy\")\n",
        "    # plt.legend()\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the Model (Test Loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, X_test, Y_test, sequence_length, training_length=100):\n",
        "    X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "    Y_test = tf.convert_to_tensor(Y_test, dtype=tf.int32)\n",
        "    \n",
        "    test_loss = 0\n",
        "    hidden_state = None\n",
        "    num_test_segments = sequence_length // training_length\n",
        "\n",
        "    all_test_preds = []  # Collect all predictions for final accuracy\n",
        "    all_true_test = []   # Collect all true labels for final accuracy\n",
        "\n",
        "    for i in range(num_test_segments):\n",
        "        start = i * training_length\n",
        "        end = start + training_length\n",
        "        \n",
        "        X_segment = X_test[:, start:end, :]\n",
        "        Y_segment = tf.argmax(Y_test[:, start:end, :], axis=-1)\n",
        "\n",
        "        if hidden_state is None:\n",
        "            output, hidden_state = model(X_segment)  # First segment\n",
        "        else:\n",
        "            output, hidden_state = model(X_segment, initial_state=hidden_state)  # Keep state\n",
        "\n",
        "        segment_loss = model.loss_fn(Y_segment, output).numpy()\n",
        "        test_loss += segment_loss\n",
        "\n",
        "        test_preds = np.argmax(output.numpy(), axis=-1)\n",
        "        true_test = Y_segment.numpy()\n",
        "\n",
        "        # Collect for final accuracy calculation\n",
        "        all_test_preds.append(test_preds)\n",
        "        all_true_test.append(true_test)\n",
        "        \n",
        "        if num_test_segments > 1:\n",
        "            segment_accuracy = np.mean(test_preds == true_test)\n",
        "            print(f\"Segment {i + 1:02d}/{num_test_segments:02d} | Loss: {segment_loss:.4f} | Accuracy: {segment_accuracy:.4f}\")\n",
        "\n",
        "    # Final test loss\n",
        "    test_loss /= num_test_segments\n",
        "\n",
        "    # Combine all predictions and labels for final accuracy\n",
        "    all_test_preds = np.concatenate(all_test_preds, axis=1)\n",
        "    all_true_test = np.concatenate(all_true_test, axis=1)\n",
        "    test_accuracy = np.mean(all_test_preds == all_true_test)\n",
        "    \n",
        "    print(f\"\\nFinal Test Loss: {test_loss:.4f} | Final Test Accuracy: {test_accuracy:.4f}\")\n",
        "    \n",
        "    return (test_loss, test_accuracy)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up the Copy Task Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnUjvLTtUys",
        "outputId": "0d895fad-60ab-4637-cf95-2df4bc18466c"
      },
      "outputs": [],
      "source": [
        "def run_benchmark():\n",
        "    vocabulary = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    delimiter = 10\n",
        "    sequence_lengths = [100, 200, 500, 1000]\n",
        "    sequence_count = 100\n",
        "    total_delimiters = 3 # Adds a few delimiters to make it clear that its the end of the sequence.\n",
        "    \n",
        "    # Hyper-parameters used across each model\n",
        "    input_size = len(vocabulary) + 1 # Vocabulary tokens + delimiter token\n",
        "    hidden_size = 128\n",
        "    training_length = 100\n",
        "    total_epochs = 10\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.01\n",
        "    \n",
        "    # Storing and printing the training and testing status of each trial along with some analytical information.\n",
        "    total_trials = 3\n",
        "    metrics = {\"Standard LSTM\": [], \"Multiplicative LSTM\": [], \"Standard GRU\": [], \"Multiplicative GRU\": []}\n",
        "    \n",
        "    for model_name, model_metrics in metrics.items():\n",
        "        for sequence in sequence_lengths:\n",
        "            metrics[model_name].append([])\n",
        "    \n",
        "    for trial in range(total_trials):\n",
        "        print(f\"Running Trial {trial+1}/{total_trials} ================================================================================\")\n",
        "        for i, sequence_length in enumerate(sequence_lengths):\n",
        "            models = {\n",
        "                \"Standard LSTM\": StandardLSTM(input_size=input_size,\n",
        "                                            hidden_size=hidden_size,\n",
        "                                            epochs=total_epochs,\n",
        "                                            batch_size=batch_size,\n",
        "                                            learning_rate=learning_rate,\n",
        "                                            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                            loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)),\n",
        "                \n",
        "                \"Multiplicative LSTM\": MultiplicativeLSTM(input_size=input_size,\n",
        "                                            hidden_size=hidden_size,\n",
        "                                            epochs=total_epochs,\n",
        "                                            batch_size=batch_size,\n",
        "                                            learning_rate=learning_rate,\n",
        "                                            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                            loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)),\n",
        "                \n",
        "                \"Standard GRU\": StandardGRU(input_size=input_size,\n",
        "                                            hidden_size=hidden_size,\n",
        "                                            epochs=total_epochs,\n",
        "                                            batch_size=batch_size,\n",
        "                                            learning_rate=learning_rate,\n",
        "                                            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                            loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)),\n",
        "                \n",
        "                \"Multiplicative GRU\": MultiplicativeGRU(input_size=input_size,\n",
        "                                            hidden_size=hidden_size,\n",
        "                                            epochs=total_epochs,\n",
        "                                            batch_size=batch_size,\n",
        "                                            learning_rate=learning_rate,\n",
        "                                            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                            loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "            }\n",
        "            \n",
        "            for model_name, model in models.items():\n",
        "                print(f\"Training {model_name}:\")\n",
        "                X_train, X_val, X_test, Y_train, Y_val, Y_test = generate_dataset_splits(sequence_count,\n",
        "                                                                                         input_size,\n",
        "                                                                                         training_length,\n",
        "                                                                                         sequence_length,\n",
        "                                                                                         delimiter,\n",
        "                                                                                         total_delimiters)\n",
        "                train_model(model, X_train, Y_train, X_val, Y_val)\n",
        "                print(f\"\\nTest Sequence Length: {sequence_length}\")\n",
        "                print(f'Testing {model_name}:')\n",
        "                result_metrics = test_model(model, X_test, Y_test, sequence_length, training_length)\n",
        "                metrics[model_name][i].append(result_metrics) # stores (loss, accuracy) pair\n",
        "                print(\"-------------------------------------------------------------------------------------------------------\")\n",
        "        \n",
        "            print(f\"*** Final metrics report for Trial {trial+1}/{total_trials} on sequence length of {sequence_length} ***\")\n",
        "            for model_name, model_metrics in metrics.items():\n",
        "                print(model_name)\n",
        "                print(f\"Final Test Loss: {model_metrics[i][trial][0]:.4f}\")\n",
        "                print(f\"Final Test Accuracy: {model_metrics[i][trial][1]:.4f}\\n\")\n",
        "\n",
        "            print(\"=======================================================================================================\")\n",
        "            \n",
        "    # Reports the mean performance (accuracy) and standard error of each model for each sequence across the total trials.\n",
        "    print(\"\\n===================================== Final Metrics Report =====================================\\n\")\n",
        "\n",
        "    # Define a format for the output\n",
        "    header_format = \"{:<22} {:<22} {:<22} {:<22}\"  # Adjust the width for better formatting\n",
        "    data_format = \"{:<22} {:<22} {:<22.4f} {:<22.4f}\"  # Ensure enough space for standard error\n",
        "\n",
        "    # Print header for the report\n",
        "    print(header_format.format(\"Model\", \"Sequence Length\", \"Mean Accuracy\", \"Standard Error\"))\n",
        "\n",
        "    # Loop through the models and their metrics\n",
        "    for model_name, model_metrics in metrics.items():\n",
        "        for i, sequence_length in enumerate(sequence_lengths):\n",
        "            accuracies = [trial[1] for trial in model_metrics[i]]  # Extract accuracy values\n",
        "            mean_accuracy = np.mean(accuracies)\n",
        "            std_error = np.std(accuracies) / np.sqrt(len(accuracies))  # Standard error = std deviation / sqrt(n)\n",
        "\n",
        "            # Print the model and its corresponding metrics for each sequence length\n",
        "            print(data_format.format(model_name, sequence_length, mean_accuracy, std_error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running Copy Task and Analyzing the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Trial 1/3 ================================================================================\n",
            "Training Standard LSTM:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-16 15:48:38.960703: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Training Loss: 2.2894 | Val Loss: 2.1263 | Training Accuracy: 0.3374 | Val Accuracy: 0.3740\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Standard LSTM:\n",
            "\n",
            "Final Test Loss: 2.1490 | Final Test Accuracy: 0.3499\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative LSTM:\n",
            "Epoch 01 | Training Loss: 2.2814 | Val Loss: 2.1318 | Training Accuracy: 0.3180 | Val Accuracy: 0.3246\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Multiplicative LSTM:\n",
            "\n",
            "Final Test Loss: 2.1491 | Final Test Accuracy: 0.3059\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Standard GRU:\n",
            "Epoch 01 | Training Loss: 2.3749 | Val Loss: 2.0742 | Training Accuracy: 0.2937 | Val Accuracy: 0.3863\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Standard GRU:\n",
            "\n",
            "Final Test Loss: 2.0735 | Final Test Accuracy: 0.3934\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative GRU:\n",
            "Epoch 01 | Training Loss: 2.3172 | Val Loss: 2.0724 | Training Accuracy: 0.2597 | Val Accuracy: 0.2945\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Multiplicative GRU:\n",
            "\n",
            "Final Test Loss: 2.1069 | Final Test Accuracy: 0.2696\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "*** Final metrics report for Trial 1/3 on sequence length of 100 ***\n",
            "Standard LSTM\n",
            "Final Test Loss: 2.1490\n",
            "Final Test Accuracy: 0.3499\n",
            "\n",
            "Multiplicative LSTM\n",
            "Final Test Loss: 2.1491\n",
            "Final Test Accuracy: 0.3059\n",
            "\n",
            "Standard GRU\n",
            "Final Test Loss: 2.0735\n",
            "Final Test Accuracy: 0.3934\n",
            "\n",
            "Multiplicative GRU\n",
            "Final Test Loss: 2.1069\n",
            "Final Test Accuracy: 0.2696\n",
            "\n",
            "=======================================================================================================\n",
            "Training Standard LSTM:\n",
            "Epoch 01 | Training Loss: 2.2590 | Val Loss: 2.0924 | Training Accuracy: 0.3592 | Val Accuracy: 0.2662\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Standard LSTM:\n",
            "Segment 01/02 | Loss: 2.1133 | Accuracy: 0.2402\n",
            "Segment 02/02 | Loss: 2.1088 | Accuracy: 0.2517\n",
            "\n",
            "Final Test Loss: 2.1110 | Final Test Accuracy: 0.2460\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative LSTM:\n",
            "Epoch 01 | Training Loss: 2.2680 | Val Loss: 2.0428 | Training Accuracy: 0.2063 | Val Accuracy: 0.3345\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Multiplicative LSTM:\n",
            "Segment 01/02 | Loss: 2.0776 | Accuracy: 0.3106\n",
            "Segment 02/02 | Loss: 2.0777 | Accuracy: 0.3119\n",
            "\n",
            "Final Test Loss: 2.0777 | Final Test Accuracy: 0.3113\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Standard GRU:\n",
            "Epoch 01 | Training Loss: 2.2763 | Val Loss: 1.9617 | Training Accuracy: 0.4369 | Val Accuracy: 0.3467\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Standard GRU:\n",
            "Segment 01/02 | Loss: 1.9830 | Accuracy: 0.3280\n",
            "Segment 02/02 | Loss: 1.9901 | Accuracy: 0.3169\n",
            "\n",
            "Final Test Loss: 1.9866 | Final Test Accuracy: 0.3225\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative GRU:\n",
            "Epoch 01 | Training Loss: 2.2554 | Val Loss: 1.9643 | Training Accuracy: 0.2767 | Val Accuracy: 0.3906\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Multiplicative GRU:\n",
            "Segment 01/02 | Loss: 2.0028 | Accuracy: 0.3749\n",
            "Segment 02/02 | Loss: 1.9986 | Accuracy: 0.3728\n",
            "\n",
            "Final Test Loss: 2.0007 | Final Test Accuracy: 0.3739\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "*** Final metrics report for Trial 1/3 on sequence length of 200 ***\n",
            "Standard LSTM\n",
            "Final Test Loss: 2.1110\n",
            "Final Test Accuracy: 0.2460\n",
            "\n",
            "Multiplicative LSTM\n",
            "Final Test Loss: 2.0777\n",
            "Final Test Accuracy: 0.3113\n",
            "\n",
            "Standard GRU\n",
            "Final Test Loss: 1.9866\n",
            "Final Test Accuracy: 0.3225\n",
            "\n",
            "Multiplicative GRU\n",
            "Final Test Loss: 2.0007\n",
            "Final Test Accuracy: 0.3739\n",
            "\n",
            "=======================================================================================================\n",
            "Running Trial 2/3 ================================================================================\n",
            "Training Standard LSTM:\n",
            "Epoch 01 | Training Loss: 2.3084 | Val Loss: 2.1493 | Training Accuracy: 0.3034 | Val Accuracy: 0.3279\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Standard LSTM:\n",
            "\n",
            "Final Test Loss: 2.1688 | Final Test Accuracy: 0.3105\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative LSTM:\n",
            "Epoch 01 | Training Loss: 2.2606 | Val Loss: 2.0692 | Training Accuracy: 0.2379 | Val Accuracy: 0.3175\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Multiplicative LSTM:\n",
            "\n",
            "Final Test Loss: 2.0965 | Final Test Accuracy: 0.2930\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Standard GRU:\n",
            "Epoch 01 | Training Loss: 2.3128 | Val Loss: 2.0611 | Training Accuracy: 0.3519 | Val Accuracy: 0.3712\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Standard GRU:\n",
            "\n",
            "Final Test Loss: 2.0780 | Final Test Accuracy: 0.3497\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative GRU:\n",
            "Epoch 01 | Training Loss: 2.3709 | Val Loss: 2.1151 | Training Accuracy: 0.2112 | Val Accuracy: 0.2260\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Multiplicative GRU:\n",
            "\n",
            "Final Test Loss: 2.1538 | Final Test Accuracy: 0.2033\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "*** Final metrics report for Trial 2/3 on sequence length of 100 ***\n",
            "Standard LSTM\n",
            "Final Test Loss: 2.1688\n",
            "Final Test Accuracy: 0.3105\n",
            "\n",
            "Multiplicative LSTM\n",
            "Final Test Loss: 2.0965\n",
            "Final Test Accuracy: 0.2930\n",
            "\n",
            "Standard GRU\n",
            "Final Test Loss: 2.0780\n",
            "Final Test Accuracy: 0.3497\n",
            "\n",
            "Multiplicative GRU\n",
            "Final Test Loss: 2.1538\n",
            "Final Test Accuracy: 0.2033\n",
            "\n",
            "=======================================================================================================\n",
            "Training Standard LSTM:\n",
            "Epoch 01 | Training Loss: 2.2746 | Val Loss: 2.1028 | Training Accuracy: 0.2743 | Val Accuracy: 0.2839\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Standard LSTM:\n",
            "Segment 01/02 | Loss: 2.1271 | Accuracy: 0.2629\n",
            "Segment 02/02 | Loss: 2.1239 | Accuracy: 0.2706\n",
            "\n",
            "Final Test Loss: 2.1255 | Final Test Accuracy: 0.2667\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative LSTM:\n",
            "Epoch 01 | Training Loss: 2.2581 | Val Loss: 2.0397 | Training Accuracy: 0.2306 | Val Accuracy: 0.2778\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Multiplicative LSTM:\n",
            "Segment 01/02 | Loss: 2.0792 | Accuracy: 0.2578\n",
            "Segment 02/02 | Loss: 2.0863 | Accuracy: 0.2594\n",
            "\n",
            "Final Test Loss: 2.0827 | Final Test Accuracy: 0.2586\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Standard GRU:\n",
            "Epoch 01 | Training Loss: 2.3696 | Val Loss: 2.1220 | Training Accuracy: 0.2379 | Val Accuracy: 0.3230\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Standard GRU:\n",
            "Segment 01/02 | Loss: 2.1303 | Accuracy: 0.3009\n",
            "Segment 02/02 | Loss: 2.1270 | Accuracy: 0.3029\n",
            "\n",
            "Final Test Loss: 2.1287 | Final Test Accuracy: 0.3019\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative GRU:\n",
            "Epoch 01 | Training Loss: 2.2888 | Val Loss: 2.1001 | Training Accuracy: 0.2573 | Val Accuracy: 0.2703\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Multiplicative GRU:\n",
            "Segment 01/02 | Loss: 2.1006 | Accuracy: 0.2907\n",
            "Segment 02/02 | Loss: 2.1017 | Accuracy: 0.2810\n",
            "\n",
            "Final Test Loss: 2.1012 | Final Test Accuracy: 0.2858\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "*** Final metrics report for Trial 2/3 on sequence length of 200 ***\n",
            "Standard LSTM\n",
            "Final Test Loss: 2.1255\n",
            "Final Test Accuracy: 0.2667\n",
            "\n",
            "Multiplicative LSTM\n",
            "Final Test Loss: 2.0827\n",
            "Final Test Accuracy: 0.2586\n",
            "\n",
            "Standard GRU\n",
            "Final Test Loss: 2.1287\n",
            "Final Test Accuracy: 0.3019\n",
            "\n",
            "Multiplicative GRU\n",
            "Final Test Loss: 2.1012\n",
            "Final Test Accuracy: 0.2858\n",
            "\n",
            "=======================================================================================================\n",
            "Running Trial 3/3 ================================================================================\n",
            "Training Standard LSTM:\n",
            "Epoch 01 | Training Loss: 2.2971 | Val Loss: 2.1411 | Training Accuracy: 0.2743 | Val Accuracy: 0.2407\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Standard LSTM:\n",
            "\n",
            "Final Test Loss: 2.1506 | Final Test Accuracy: 0.2208\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative LSTM:\n",
            "Epoch 01 | Training Loss: 2.2428 | Val Loss: 2.0372 | Training Accuracy: 0.3252 | Val Accuracy: 0.3628\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Multiplicative LSTM:\n",
            "\n",
            "Final Test Loss: 2.0527 | Final Test Accuracy: 0.3515\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Standard GRU:\n",
            "Epoch 01 | Training Loss: 2.3485 | Val Loss: 2.0616 | Training Accuracy: 0.2961 | Val Accuracy: 0.3497\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Standard GRU:\n",
            "\n",
            "Final Test Loss: 2.0766 | Final Test Accuracy: 0.3369\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative GRU:\n",
            "Epoch 01 | Training Loss: 2.2965 | Val Loss: 1.9634 | Training Accuracy: 0.3204 | Val Accuracy: 0.3678\n",
            "\n",
            "Test Sequence Length: 100\n",
            "Testing Multiplicative GRU:\n",
            "\n",
            "Final Test Loss: 1.9907 | Final Test Accuracy: 0.3466\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "*** Final metrics report for Trial 3/3 on sequence length of 100 ***\n",
            "Standard LSTM\n",
            "Final Test Loss: 2.1506\n",
            "Final Test Accuracy: 0.2208\n",
            "\n",
            "Multiplicative LSTM\n",
            "Final Test Loss: 2.0527\n",
            "Final Test Accuracy: 0.3515\n",
            "\n",
            "Standard GRU\n",
            "Final Test Loss: 2.0766\n",
            "Final Test Accuracy: 0.3369\n",
            "\n",
            "Multiplicative GRU\n",
            "Final Test Loss: 1.9907\n",
            "Final Test Accuracy: 0.3466\n",
            "\n",
            "=======================================================================================================\n",
            "Training Standard LSTM:\n",
            "Epoch 01 | Training Loss: 2.2871 | Val Loss: 2.1276 | Training Accuracy: 0.3252 | Val Accuracy: 0.2823\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Standard LSTM:\n",
            "Segment 01/02 | Loss: 2.1287 | Accuracy: 0.2933\n",
            "Segment 02/02 | Loss: 2.1290 | Accuracy: 0.3009\n",
            "\n",
            "Final Test Loss: 2.1288 | Final Test Accuracy: 0.2971\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative LSTM:\n",
            "Epoch 01 | Training Loss: 2.2055 | Val Loss: 1.9196 | Training Accuracy: 0.3714 | Val Accuracy: 0.3990\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Multiplicative LSTM:\n",
            "Segment 01/02 | Loss: 1.9551 | Accuracy: 0.3857\n",
            "Segment 02/02 | Loss: 1.9440 | Accuracy: 0.3994\n",
            "\n",
            "Final Test Loss: 1.9496 | Final Test Accuracy: 0.3926\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Standard GRU:\n",
            "Epoch 01 | Training Loss: 2.2699 | Val Loss: 1.9421 | Training Accuracy: 0.2840 | Val Accuracy: 0.4298\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Standard GRU:\n",
            "Segment 01/02 | Loss: 1.9621 | Accuracy: 0.4136\n",
            "Segment 02/02 | Loss: 1.9678 | Accuracy: 0.4099\n",
            "\n",
            "Final Test Loss: 1.9650 | Final Test Accuracy: 0.4118\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Training Multiplicative GRU:\n",
            "Epoch 01 | Training Loss: 2.2799 | Val Loss: 1.9971 | Training Accuracy: 0.2961 | Val Accuracy: 0.3082\n",
            "\n",
            "Test Sequence Length: 200\n",
            "Testing Multiplicative GRU:\n",
            "Segment 01/02 | Loss: 2.0441 | Accuracy: 0.2982\n",
            "Segment 02/02 | Loss: 2.0505 | Accuracy: 0.2874\n",
            "\n",
            "Final Test Loss: 2.0473 | Final Test Accuracy: 0.2928\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "*** Final metrics report for Trial 3/3 on sequence length of 200 ***\n",
            "Standard LSTM\n",
            "Final Test Loss: 2.1288\n",
            "Final Test Accuracy: 0.2971\n",
            "\n",
            "Multiplicative LSTM\n",
            "Final Test Loss: 1.9496\n",
            "Final Test Accuracy: 0.3926\n",
            "\n",
            "Standard GRU\n",
            "Final Test Loss: 1.9650\n",
            "Final Test Accuracy: 0.4118\n",
            "\n",
            "Multiplicative GRU\n",
            "Final Test Loss: 2.0473\n",
            "Final Test Accuracy: 0.2928\n",
            "\n",
            "=======================================================================================================\n",
            "\n",
            "=== FINAL METRICS REPORT ===\n",
            "Model                  Sequence Length        Mean Accuracy          Standard Error        \n",
            "Standard LSTM          100                    0.2937                 0.0312                \n",
            "Standard LSTM          200                    0.2699                 0.0121                \n",
            "Multiplicative LSTM    100                    0.3168                 0.0145                \n",
            "Multiplicative LSTM    200                    0.3208                 0.0318                \n",
            "Standard GRU           100                    0.3600                 0.0140                \n",
            "Standard GRU           200                    0.3454                 0.0275                \n",
            "Multiplicative GRU     100                    0.2732                 0.0338                \n",
            "Multiplicative GRU     200                    0.3175                 0.0231                \n"
          ]
        }
      ],
      "source": [
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Analyzing Copy Task Results\n",
        "### Analysis of Results\n",
        "### Observations on Multiplicative Effects"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
