{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 3: LSTM and GRU vs. Multiplicative Variations\n",
        "\n",
        "### Maxim Ryabinov (U02204083)\n",
        "### CAP4641: Natural Language Processing \n",
        "### Instructor: Dr. Ankur Mali \n",
        "### University of South Florida (Spring 2025)\n",
        "\n",
        "---\n",
        "\n",
        "# Description\n",
        "\n",
        "In this assignment, I implemented Standard LSTM RNN, Standard GRU RNN, Multiplicative LSTM RNN, and Multiplicative GRU RNN. My choice for the machine learning library used in this notebook is TensorFlow.\n",
        "\n",
        "Below, you will find an implementation for each recurrent neural network architecture, all following a set of model equations that each of the architectures are based off of.\n",
        "\n",
        "Lastly, in order to show robustness and highlight the differences in gating mechanisms between the architectures, the Copy Task is carried out. Each model is ran through this task using varying lengths of sequences that contain a randomly generated set of characters (sequences lengths: `{100, 200, 500, 1000}`).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Defining each RNN Architecture\n",
        "\n",
        "### Standard LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        i_t = tf.sigmoid(tf.matmul(x_t, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_t, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_t, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_t, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = StandardLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights and bias\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights and bias\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights and bias\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights and bias\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Multiplicative extension weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        # Multiplicative Extension\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        i_t = tf.sigmoid(tf.matmul(x_cap, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_cap, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_cap, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_cap, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "class MultiplicativeLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = MultiplicativeLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard GRU RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardGRUCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate weights\n",
        "        self.W_z = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_z = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_z = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Reset gate weights\n",
        "        self.W_r = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_r = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_r = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Candidate hidden state weights\n",
        "        self.W_h = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_h = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_h = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        # Update gate\n",
        "        z_t = tf.sigmoid(tf.matmul(x_t, self.W_z) + tf.matmul(h_prev, self.U_z) + self.b_z)\n",
        "        \n",
        "        # Reset gate\n",
        "        r_t = tf.sigmoid(tf.matmul(x_t, self.W_r) + tf.matmul(h_prev, self.U_r) + self.b_r)\n",
        "        \n",
        "        # Candidate hidden state\n",
        "        h_hat = tf.tanh(tf.matmul(x_t, self.W_h) + tf.matmul(r_t * h_prev, self.U_h) + self.b_h)\n",
        "        \n",
        "        # New hidden state\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_hat\n",
        "        \n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardGRU(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = StandardGRUCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.gru_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative GRU RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeGRUCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate weights\n",
        "        self.W_z = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_z = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_z = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Reset gate weights\n",
        "        self.W_r = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_r = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_r = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Candidate hidden state weights\n",
        "        self.W_h = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_h = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_h = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Multiplicative extension weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        # Memory matrix introduction\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        # Update gate\n",
        "        z_t = tf.sigmoid(tf.matmul(x_cap, self.W_z) + tf.matmul(h_prev, self.U_z) + self.b_z)\n",
        "        \n",
        "        # Reset gate\n",
        "        r_t = tf.sigmoid(tf.matmul(x_cap, self.W_r) + tf.matmul(h_prev, self.U_r) + self.b_r)\n",
        "        \n",
        "        # Candidate hidden state\n",
        "        h_hat = tf.tanh(tf.matmul(x_cap, self.W_h) + tf.matmul(r_t * h_prev, self.U_h) + self.b_h)\n",
        "        \n",
        "        # New hidden state\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_hat\n",
        "        \n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class MultiplicativeGRU(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = MultiplicativeGRUCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.gru_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Train, Test, and Validation Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Copy Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnUjvLTtUys",
        "outputId": "0d895fad-60ab-4637-cf95-2df4bc18466c"
      },
      "outputs": [],
      "source": [
        "def benchmark_model(model, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    X = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(X)\n",
        "            loss = loss_fn(output, Y)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        print(f\"Epoch {epoch+1} | Loss: {loss.numpy():.6f}\")\n",
        "    \n",
        "    return time.time() - start_time\n",
        "\n",
        "\n",
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    seq_length = 20\n",
        "    batch_size = 32\n",
        "    input_size = 10\n",
        "    hidden_size = 128\n",
        "    num_epochs = 10\n",
        "\n",
        "    np.random.seed(123)\n",
        "    # TODO: set all random seeds, currently results still unpredictable\n",
        "    X_train = np.random.rand(1000, seq_length, input_size).astype(np.float32)\n",
        "    Y_train = X_train.copy()\n",
        "\n",
        "    lstm_time = benchmark_model(StandardLSTM(input_size, hidden_size), X_train, Y_train, num_epochs)\n",
        "    print(\"========================================================\")\n",
        "    mul_lstm_time = benchmark_model(MultiplicativeLSTM(input_size, hidden_size), X_train, Y_train, num_epochs)\n",
        "    print(\"========================================================\")\n",
        "    gru_time = benchmark_model(StandardGRU(input_size, hidden_size), X_train, Y_train, num_epochs)\n",
        "    print(\"========================================================\")\n",
        "    mul_gru_time = benchmark_model(MultiplicativeGRU(input_size, hidden_size), X_train, Y_train, num_epochs)\n",
        "    print(\"========================================================\\n\")\n",
        "\n",
        "    print(f\"Standard LSTM Time: {lstm_time:.4f} seconds\")\n",
        "    print(f\"Multiplicative LSTM Time: {mul_lstm_time:.4f} seconds\")\n",
        "    print(f\"Standard GRU Time: {gru_time:.4f} seconds\")\n",
        "    print(f\"Multiplicative GRU Time: {mul_gru_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 0.334536\n",
            "Epoch 2 | Loss: 0.225049\n",
            "Epoch 3 | Loss: 0.121364\n",
            "Epoch 4 | Loss: 0.103572\n",
            "Epoch 5 | Loss: 0.093541\n",
            "Epoch 6 | Loss: 0.091538\n",
            "Epoch 7 | Loss: 0.089722\n",
            "Epoch 8 | Loss: 0.080486\n",
            "Epoch 9 | Loss: 0.078451\n",
            "Epoch 10 | Loss: 0.079309\n",
            "========================================================\n",
            "Epoch 1 | Loss: 0.334641\n",
            "Epoch 2 | Loss: 0.310167\n",
            "Epoch 3 | Loss: 0.247393\n",
            "Epoch 4 | Loss: 0.216494\n",
            "Epoch 5 | Loss: 0.184076\n",
            "Epoch 6 | Loss: 0.209777\n",
            "Epoch 7 | Loss: 0.207175\n",
            "Epoch 8 | Loss: 0.192802\n",
            "Epoch 9 | Loss: 0.169475\n",
            "Epoch 10 | Loss: 0.136930\n",
            "========================================================\n",
            "Epoch 1 | Loss: 0.346141\n",
            "Epoch 2 | Loss: 0.164280\n",
            "Epoch 3 | Loss: 0.181863\n",
            "Epoch 4 | Loss: 0.092724\n",
            "Epoch 5 | Loss: 0.120933\n",
            "Epoch 6 | Loss: 0.109495\n",
            "Epoch 7 | Loss: 0.087944\n",
            "Epoch 8 | Loss: 0.086077\n",
            "Epoch 9 | Loss: 0.086548\n",
            "Epoch 10 | Loss: 0.077663\n",
            "========================================================\n",
            "Epoch 1 | Loss: 0.334098\n",
            "Epoch 2 | Loss: 0.294541\n",
            "Epoch 3 | Loss: 0.170096\n",
            "Epoch 4 | Loss: 1.734606\n",
            "Epoch 5 | Loss: 0.122123\n",
            "Epoch 6 | Loss: 0.204261\n",
            "Epoch 7 | Loss: 0.238248\n",
            "Epoch 8 | Loss: 0.247933\n",
            "Epoch 9 | Loss: 0.246099\n",
            "Epoch 10 | Loss: 0.237181\n",
            "========================================================\n",
            "\n",
            "Standard LSTM Time: 4.0151 seconds\n",
            "Multiplicative LSTM Time: 4.4788 seconds\n",
            "Standard GRU Time: 3.2121 seconds\n",
            "Multiplicative GRU Time: 3.6585 seconds\n"
          ]
        }
      ],
      "source": [
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TODO\n",
        "\n",
        "- Add accuracy measurement\n",
        "- Look into vanishing/exploding gradients and how these can be displayed (need this for analysis later)\n",
        "- Do copy task, figure out what a delimitor is\n",
        "- Remember, 3 copy tasks need to be ran on each model so that mean accuracy and standard error can be measured\n",
        "- look into cross entropy??"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
