{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 3: LSTM and GRU vs. Multiplicative Variations\n",
        "\n",
        "### Maxim Ryabinov (U02204083)\n",
        "### CAP4641: Natural Language Processing \n",
        "### Instructor: Dr. Ankur Mali \n",
        "### University of South Florida (Spring 2025)\n",
        "\n",
        "---\n",
        "\n",
        "# Description\n",
        "\n",
        "In this assignment, I implemented Standard LSTM RNN, Standard GRU RNN, Multiplicative LSTM RNN, and Multiplicative GRU RNN. My choice for the machine learning library used in this notebook is TensorFlow.\n",
        "\n",
        "Below, you will find an implementation for each recurrent neural network architecture, all following a set of model equations that each of the architectures are based off of.\n",
        "\n",
        "Lastly, in order to show robustness and highlight the differences in gating mechanisms between the architectures, the Copy Task is carried out. Each model is ran through this task using varying lengths of sequences that contain a randomly generated set of characters (sequences lengths: `{100, 200, 500, 1000}`).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-15 22:48:59.698916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742093339.715832   42307 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742093339.720818   42307 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-15 22:48:59.737983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Defining each RNN Architecture\n",
        "\n",
        "### Standard LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        i_t = tf.sigmoid(tf.matmul(x_t, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_t, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_t, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_t, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = StandardLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state = None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "            c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h, c = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), (h, c)  # [batch_size, seq_length, input_size] and h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights and bias\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights and bias\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights and bias\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights and bias\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Multiplicative extension weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        # Multiplicative Extension\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        i_t = tf.sigmoid(tf.matmul(x_cap, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_cap, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_cap, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_cap, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "class MultiplicativeLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = MultiplicativeLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "\n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "            c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h, c = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), (h, c)  # [batch_size, seq_length, input_size] and h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard GRU RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardGRUCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate weights\n",
        "        self.W_z = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_z = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_z = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Reset gate weights\n",
        "        self.W_r = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_r = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_r = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Candidate hidden state weights\n",
        "        self.W_h = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_h = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_h = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        # Update gate\n",
        "        z_t = tf.sigmoid(tf.matmul(x_t, self.W_z) + tf.matmul(h_prev, self.U_z) + self.b_z)\n",
        "        \n",
        "        # Reset gate\n",
        "        r_t = tf.sigmoid(tf.matmul(x_t, self.W_r) + tf.matmul(h_prev, self.U_r) + self.b_r)\n",
        "        \n",
        "        # Candidate hidden state\n",
        "        h_hat = tf.tanh(tf.matmul(x_t, self.W_h) + tf.matmul(r_t * h_prev, self.U_h) + self.b_h)\n",
        "        \n",
        "        # New hidden state\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_hat\n",
        "        \n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardGRU(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = StandardGRUCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.gru_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), h  # [batch_size, seq_length, input_size] and h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative GRU RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeGRUCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate weights\n",
        "        self.W_z = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_z = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_z = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Reset gate weights\n",
        "        self.W_r = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_r = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_r = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Candidate hidden state weights\n",
        "        self.W_h = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_h = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_h = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Multiplicative extension weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        # Memory matrix introduction\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        # Update gate\n",
        "        z_t = tf.sigmoid(tf.matmul(x_cap, self.W_z) + tf.matmul(h_prev, self.U_z) + self.b_z)\n",
        "        \n",
        "        # Reset gate\n",
        "        r_t = tf.sigmoid(tf.matmul(x_cap, self.W_r) + tf.matmul(h_prev, self.U_r) + self.b_r)\n",
        "        \n",
        "        # Candidate hidden state\n",
        "        h_hat = tf.tanh(tf.matmul(x_cap, self.W_h) + tf.matmul(r_t * h_prev, self.U_h) + self.b_h)\n",
        "        \n",
        "        # New hidden state\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_hat\n",
        "        \n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class MultiplicativeGRU(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = MultiplicativeGRUCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.gru_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), h  # [batch_size, seq_length, input_size] and h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Train, Test, and Validation Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Copy Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnUjvLTtUys",
        "outputId": "0d895fad-60ab-4637-cf95-2df4bc18466c"
      },
      "outputs": [],
      "source": [
        "def benchmark_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test, sequence_length, training_length=100, epochs=10, batch_size=32, lr=0.01):\n",
        "    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y_train = tf.convert_to_tensor(Y_train, dtype=tf.float32)  # Change to int32 for sparse categorical cross-entropy\n",
        "    X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "    Y_val = tf.convert_to_tensor(Y_val, dtype=tf.float32)  # Change to int32 for sparse categorical cross-entropy\n",
        "    X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "    Y_test = tf.convert_to_tensor(Y_test, dtype=tf.float32)  # Change to int32 for sparse categorical cross-entropy\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data at the start of each epoch.\n",
        "        indices = tf.range(start=X_train.shape[0])\n",
        "        indices = tf.random.shuffle(indices)\n",
        "        X_train = tf.gather(X_train, indices)\n",
        "        Y_train = tf.gather(Y_train, indices)\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start = i * batch_size\n",
        "            end = min((i+1) * batch_size, X_train.shape[0])\n",
        "            X_batch = X_train[start:end]\n",
        "            Y_batch = Y_train[start:end]\n",
        "            \n",
        "            # Convert one-hot encoded labels to integer class labels\n",
        "            Y_batch_labels = tf.argmax(Y_batch, axis=-1)  # Shape will be (batch_size, seq_length)\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "                output, _ = model(X_batch)\n",
        "                batch_loss = loss_fn(Y_batch_labels, output)\n",
        "            gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            \n",
        "            epoch_loss += batch_loss.numpy()\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        \n",
        "        # Calculate training accuracy\n",
        "        train_preds = np.argmax(output.numpy(), axis=-1).flatten()\n",
        "        true_train = Y_batch_labels.numpy().flatten()\n",
        "        train_accuracy = np.mean(train_preds == true_train)\n",
        "        \n",
        "        # Calculate validation loss and accuracy\n",
        "        val_output, _ = model(X_val)\n",
        "        val_loss = loss_fn(tf.argmax(Y_val, axis=-1), val_output).numpy()\n",
        "        \n",
        "        val_preds = np.argmax(val_output.numpy(), axis=-1)\n",
        "        true_val = tf.argmax(Y_val, axis=-1).numpy()\n",
        "        val_accuracy = np.mean(val_preds == true_val)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "              f\"Training Accuracy: {train_accuracy:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
        "    \n",
        "    \n",
        "    # Testing Phase    \n",
        "    # test_output = model(X_test)\n",
        "    # test_loss = loss_fn(tf.argmax(Y_test, axis=-1), test_output).numpy()\n",
        "    # test_preds = np.argmax(test_output.numpy(), axis=-1)\n",
        "    # true_test = tf.argmax(Y_test, axis=-1).numpy()\n",
        "    # test_accuracy = np.mean(test_preds == true_test)\n",
        "    \n",
        "    test_loss = 0\n",
        "    hidden_state = None\n",
        "    num_test_segments = sequence_length // training_length\n",
        "\n",
        "    all_test_preds = []  # Collect all predictions for final accuracy\n",
        "    all_true_test = []   # Collect all true labels for final accuracy\n",
        "\n",
        "    for i in range(num_test_segments):\n",
        "        start = i * training_length\n",
        "        end = min((i + 1) * training_length, sequence_length)\n",
        "        \n",
        "        X_segment = X_test[:, start:end, :]\n",
        "        Y_segment = tf.argmax(Y_test[:, start:end, :], axis=-1)\n",
        "\n",
        "        if hidden_state is None:\n",
        "            output, hidden_state = model(X_segment)  # First segment\n",
        "        else:\n",
        "            output, hidden_state = model(X_segment, initial_state=hidden_state)  # Keep state\n",
        "\n",
        "        segment_loss = loss_fn(Y_segment, output).numpy()\n",
        "        test_loss += segment_loss\n",
        "\n",
        "        test_preds = np.argmax(output.numpy(), axis=-1)\n",
        "        true_test = Y_segment.numpy()\n",
        "\n",
        "        # Collect for final accuracy calculation\n",
        "        all_test_preds.append(test_preds)\n",
        "        all_true_test.append(true_test)\n",
        "        \n",
        "        if num_test_segments > 1:\n",
        "            segment_accuracy = np.mean(test_preds == true_test)\n",
        "            print(f\"Segment {i + 1}/{num_test_segments} | Loss: {segment_loss:.4f} | Accuracy: {segment_accuracy:.4f}\")\n",
        "\n",
        "    # Final test loss\n",
        "    test_loss /= num_test_segments\n",
        "\n",
        "    # Combine all predictions and labels for final accuracy\n",
        "    all_test_preds = np.concatenate(all_test_preds, axis=1)\n",
        "    all_true_test = np.concatenate(all_true_test, axis=1)\n",
        "    test_accuracy = np.mean(all_test_preds == all_true_test)\n",
        "    \n",
        "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}\")\n",
        "    \n",
        "    return time.time() - start_time\n",
        "\n",
        "\n",
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    vocabulary = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    delimiter = 10\n",
        "    \n",
        "    # Hyper-parameters used across each model\n",
        "    sequence_lengths = [100, 200, 500, 1000]\n",
        "    sequence_length = sequence_lengths[1]\n",
        "    sequence_count = 100\n",
        "    total_delimiters = 3 # Adds a few delimiters to make it clear that its the end of the sequence.\n",
        "    input_size = len(vocabulary) + total_delimiters # Vocabulary tokens + delimiter token\n",
        "    hidden_size = 128\n",
        "    training_length = 100\n",
        "    total_epochs = 10\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.01\n",
        "    \n",
        "    import os\n",
        "    import random\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Set the seed for Python\n",
        "    random.seed(123)\n",
        "\n",
        "    # Set the seed for NumPy\n",
        "    np.random.seed(123)\n",
        "\n",
        "    # Set the seed for TensorFlow\n",
        "    tf.random.set_seed(123)\n",
        "\n",
        "    # Make TensorFlow deterministic\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Force TensorFlow to use deterministic ops\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # Ensure cuDNN is deterministic\n",
        "    \n",
        "\n",
        "    # TODO: set all random seeds, currently results still unpredictable\n",
        "    # tf.random.set_seed(123)\n",
        "    # np.random.seed(123)\n",
        "    \n",
        "    # X_train = np.random.rand(seq_count, seq_length, input_size).astype(np.float32)\n",
        "    # Y_train = X_train.copy()\n",
        "    # X_val = np.random.rand(seq_count, seq_length, input_size).astype(np.float32)\n",
        "    # Y_val = X_train.copy()\n",
        "    # X_test = np.random.rand(seq_count, seq_length, input_size).astype(np.float32)\n",
        "    # Y_test = X_test.copy()\n",
        "    print(f\"Sequence length: {sequence_length}\")\n",
        "    X_train = np.random.randint(0, 10, size=(sequence_count, sequence_length, input_size)).astype(np.float32)\n",
        "    X_val = np.random.randint(0, 10, size=(sequence_count, sequence_length, input_size)).astype(np.float32)\n",
        "    X_test = np.random.randint(0, 10, size=(sequence_count, sequence_length, input_size)).astype(np.float32)\n",
        "    delimiters = np.full((sequence_count, total_delimiters, input_size), delimiter, dtype=np.float32)\n",
        "    \n",
        "    X_train = np.concatenate([X_train, delimiters], axis=1)\n",
        "    X_val = np.concatenate([X_val, delimiters], axis=1)\n",
        "    X_test = np.concatenate([X_test, delimiters], axis=1)\n",
        "    \n",
        "    Y_train = X_train.copy()\n",
        "    Y_val = X_val.copy()\n",
        "    Y_test = X_test.copy()\n",
        "\n",
        "    \n",
        "    lstm_time = benchmark_model(StandardLSTM(input_size, hidden_size), X_train, Y_train, X_val, Y_val, X_test, Y_test, sequence_length, epochs=3)\n",
        "    print(\"=======================================================================================================\")\n",
        "    mul_lstm_time = benchmark_model(MultiplicativeLSTM(input_size, hidden_size), X_train, Y_train, X_val, Y_val, X_test, Y_test, sequence_length, epochs=3)\n",
        "    print(\"=======================================================================================================\")\n",
        "    gru_time = benchmark_model(StandardGRU(input_size, hidden_size), X_train, Y_train, X_val, Y_val, X_test, Y_test, sequence_length, epochs=3)\n",
        "    print(\"=======================================================================================================\")\n",
        "    mul_gru_time = benchmark_model(MultiplicativeGRU(input_size, hidden_size), X_train, Y_train, X_val, Y_val, X_test, Y_test, sequence_length, epochs=3)\n",
        "    print(\"=======================================================================================================\\n\")\n",
        "\n",
        "    print(f\"Standard LSTM Time: {lstm_time:.4f} seconds\")\n",
        "    print(f\"Multiplicative LSTM Time: {mul_lstm_time:.4f} seconds\")\n",
        "    print(f\"Standard GRU Time: {gru_time:.4f} seconds\")\n",
        "    print(f\"Multiplicative GRU Time: {mul_gru_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence length: 200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-15 22:49:01.775886: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Training Loss: 2.4286 | Val Loss: 2.2339 | Training Accuracy: 0.1946 | Val Accuracy: 0.3462\n",
            "Epoch 02 | Training Loss: 2.0777 | Val Loss: 1.8305 | Training Accuracy: 0.4347 | Val Accuracy: 0.4958\n",
            "Epoch 03 | Training Loss: 1.6791 | Val Loss: 1.4773 | Training Accuracy: 0.5542 | Val Accuracy: 0.5825\n",
            "Segment 1/2 | Loss: 1.4937 | Accuracy: 0.5772\n",
            "Segment 2/2 | Loss: 1.4979 | Accuracy: 0.5707\n",
            "\n",
            "Test Loss: 1.4958 | Test Accuracy: 0.5739\n",
            "=======================================================================================================\n",
            "Epoch 01 | Training Loss: 2.4500 | Val Loss: 2.2650 | Training Accuracy: 0.2352 | Val Accuracy: 0.3209\n",
            "Epoch 02 | Training Loss: 2.1005 | Val Loss: 1.8735 | Training Accuracy: 0.4249 | Val Accuracy: 0.4106\n",
            "Epoch 03 | Training Loss: 1.7318 | Val Loss: 1.5439 | Training Accuracy: 0.5000 | Val Accuracy: 0.5223\n",
            "Segment 1/2 | Loss: 1.5556 | Accuracy: 0.5165\n",
            "Segment 2/2 | Loss: 1.5770 | Accuracy: 0.5090\n",
            "\n",
            "Test Loss: 1.5663 | Test Accuracy: 0.5128\n",
            "=======================================================================================================\n",
            "Epoch 01 | Training Loss: 2.4404 | Val Loss: 2.1589 | Training Accuracy: 0.3645 | Val Accuracy: 0.3128\n",
            "Epoch 02 | Training Loss: 1.9959 | Val Loss: 1.7455 | Training Accuracy: 0.5320 | Val Accuracy: 0.5170\n",
            "Epoch 03 | Training Loss: 1.6030 | Val Loss: 1.4064 | Training Accuracy: 0.5911 | Val Accuracy: 0.6271\n",
            "Segment 1/2 | Loss: 1.4073 | Accuracy: 0.6154\n",
            "Segment 2/2 | Loss: 1.4154 | Accuracy: 0.6154\n",
            "\n",
            "Test Loss: 1.4114 | Test Accuracy: 0.6154\n",
            "=======================================================================================================\n",
            "Epoch 01 | Training Loss: 2.4435 | Val Loss: 2.2882 | Training Accuracy: 0.2426 | Val Accuracy: 0.2499\n",
            "Epoch 02 | Training Loss: 2.1406 | Val Loss: 1.9470 | Training Accuracy: 0.3079 | Val Accuracy: 0.3669\n",
            "Epoch 03 | Training Loss: 1.8447 | Val Loss: 1.6633 | Training Accuracy: 0.4236 | Val Accuracy: 0.4652\n",
            "Segment 1/2 | Loss: 1.6816 | Accuracy: 0.4559\n",
            "Segment 2/2 | Loss: 1.6823 | Accuracy: 0.4497\n",
            "\n",
            "Test Loss: 1.6819 | Test Accuracy: 0.4528\n",
            "=======================================================================================================\n",
            "\n",
            "Standard LSTM Time: 16.9916 seconds\n",
            "Multiplicative LSTM Time: 20.0398 seconds\n",
            "Standard GRU Time: 14.4480 seconds\n",
            "Multiplicative GRU Time: 17.7735 seconds\n"
          ]
        }
      ],
      "source": [
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TODO\n",
        "\n",
        "- Add accuracy measurement\n",
        "- Look into vanishing/exploding gradients and how these can be displayed (need this for analysis later)\n",
        "- Do copy task, figure out what a delimitor is\n",
        "- Remember, 3 copy tasks need to be ran on each model so that mean accuracy and standard error can be measured\n",
        "- look into cross entropy??"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
