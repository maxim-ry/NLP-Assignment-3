{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 3: LSTM and GRU vs. Multiplicative Variations\n",
        "\n",
        "### Maxim Ryabinov (U02204083)\n",
        "### CAP4641: Natural Language Processing \n",
        "### Instructor: Dr. Ankur Mali \n",
        "### University of South Florida (Spring 2025)\n",
        "\n",
        "---\n",
        "\n",
        "# Description\n",
        "\n",
        "In this assignment, I implemented Standard LSTM RNN, Standard GRU RNN, Multiplicative LSTM RNN, and Multiplicative GRU RNN. My choice for the machine learning library used in this notebook is TensorFlow.\n",
        "\n",
        "Below, you will find an implementation for each recurrent neural network architecture, all following a set of model equations that each of the architectures are based off of.\n",
        "\n",
        "Lastly, in order to show robustness and highlight the differences in gating mechanisms between the architectures, the Copy Task is carried out. Each model is ran through this task using varying lengths of sequences that contain a randomly generated set of characters (sequences lengths: `{100, 200, 500, 1000}`).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "seed = 123\n",
        "\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Make TensorFlow deterministic\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Force TensorFlow to use deterministic ops\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # Ensure cuDNN is deterministic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Defining each RNN Architecture\n",
        "\n",
        "### Standard LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        i_t = tf.sigmoid(tf.matmul(x_t, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_t, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_t, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_t, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = StandardLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state = None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "            c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h, c = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), (h, c)  # [batch_size, seq_length, input_size] and h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights and bias\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights and bias\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights and bias\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights and bias\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Multiplicative extension weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        # Multiplicative Extension\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        i_t = tf.sigmoid(tf.matmul(x_cap, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_cap, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_cap, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_cap, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "class MultiplicativeLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = MultiplicativeLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "\n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "            c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h, c = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), (h, c)  # [batch_size, seq_length, input_size] and h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard GRU RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardGRUCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate weights\n",
        "        self.W_z = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_z = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_z = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Reset gate weights\n",
        "        self.W_r = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_r = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_r = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Candidate hidden state weights\n",
        "        self.W_h = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_h = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_h = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        # Update gate\n",
        "        z_t = tf.sigmoid(tf.matmul(x_t, self.W_z) + tf.matmul(h_prev, self.U_z) + self.b_z)\n",
        "        \n",
        "        # Reset gate\n",
        "        r_t = tf.sigmoid(tf.matmul(x_t, self.W_r) + tf.matmul(h_prev, self.U_r) + self.b_r)\n",
        "        \n",
        "        # Candidate hidden state\n",
        "        h_hat = tf.tanh(tf.matmul(x_t, self.W_h) + tf.matmul(r_t * h_prev, self.U_h) + self.b_h)\n",
        "        \n",
        "        # New hidden state\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_hat\n",
        "        \n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardGRU(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = StandardGRUCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.gru_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), h  # [batch_size, seq_length, input_size] and h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiplicative GRU RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeGRUCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate weights\n",
        "        self.W_z = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_z = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_z = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Reset gate weights\n",
        "        self.W_r = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_r = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_r = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Candidate hidden state weights\n",
        "        self.W_h = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_h = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_h = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Multiplicative extension weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        # Memory matrix introduction\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        # Update gate\n",
        "        z_t = tf.sigmoid(tf.matmul(x_cap, self.W_z) + tf.matmul(h_prev, self.U_z) + self.b_z)\n",
        "        \n",
        "        # Reset gate\n",
        "        r_t = tf.sigmoid(tf.matmul(x_cap, self.W_r) + tf.matmul(h_prev, self.U_r) + self.b_r)\n",
        "        \n",
        "        # Candidate hidden state\n",
        "        h_hat = tf.tanh(tf.matmul(x_cap, self.W_h) + tf.matmul(r_t * h_prev, self.U_h) + self.b_h)\n",
        "        \n",
        "        # New hidden state\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_hat\n",
        "        \n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class MultiplicativeGRU(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = MultiplicativeGRUCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X, initial_state=None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        \n",
        "        if initial_state == None:\n",
        "            h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        else:\n",
        "            h = initial_state\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.gru_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1), h  # [batch_size, seq_length, input_size] and h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Running the Copy Task\n",
        "### Train, Test, and Validation Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset_splits(sequence_count, input_size, training_length, sequence_length, delimiter, total_delimiters):\n",
        "    X_train = np.random.randint(0, 10, size=(sequence_count, training_length, input_size)).astype(np.float32)\n",
        "    X_val = np.random.randint(0, 10, size=(sequence_count, training_length, input_size)).astype(np.float32)\n",
        "    X_test = np.random.randint(0, 10, size=(sequence_count, sequence_length, input_size)).astype(np.float32)\n",
        "    delimiters = np.full((sequence_count, total_delimiters, input_size), delimiter, dtype=np.float32)\n",
        "    \n",
        "    X_train = np.concatenate([X_train, delimiters], axis=1)\n",
        "    X_val = np.concatenate([X_val, delimiters], axis=1)\n",
        "    X_test = np.concatenate([X_test, delimiters], axis=1)\n",
        "    \n",
        "    Y_train = X_train.copy()\n",
        "    Y_val = X_val.copy()\n",
        "    Y_test = X_test.copy()\n",
        "    \n",
        "    return X_train, X_val, X_test, Y_train, Y_val, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Validating the Model (Training Loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, X_train, Y_train, X_val, Y_val, epochs=10, batch_size=32, lr=0.01):\n",
        "    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y_train = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "    X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "    Y_val = tf.convert_to_tensor(Y_val, dtype=tf.float32)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data at the start of each epoch.\n",
        "        indices = tf.range(start=X_train.shape[0])\n",
        "        indices = tf.random.shuffle(indices)\n",
        "        X_train = tf.gather(X_train, indices)\n",
        "        Y_train = tf.gather(Y_train, indices)\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start = i * batch_size\n",
        "            end = min((i+1) * batch_size, X_train.shape[0])\n",
        "            X_batch = X_train[start:end]\n",
        "            Y_batch = Y_train[start:end]\n",
        "            \n",
        "            # Convert one-hot encoded labels to integer class labels\n",
        "            Y_batch_labels = tf.argmax(Y_batch, axis=-1)  # Shape will be (batch_size, seq_length)\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "                output, _ = model(X_batch)\n",
        "                batch_loss = loss_fn(Y_batch_labels, output)\n",
        "            gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            \n",
        "            epoch_loss += batch_loss.numpy()\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "        \n",
        "        # Calculate training accuracy\n",
        "        train_preds = np.argmax(output.numpy(), axis=-1).flatten()\n",
        "        true_train = Y_batch_labels.numpy().flatten()\n",
        "        train_accuracy = np.mean(train_preds == true_train)\n",
        "        \n",
        "        # Calculate validation loss and accuracy\n",
        "        val_output, _ = model(X_val)\n",
        "        val_loss = loss_fn(tf.argmax(Y_val, axis=-1), val_output).numpy()\n",
        "        \n",
        "        val_preds = np.argmax(val_output.numpy(), axis=-1)\n",
        "        true_val = tf.argmax(Y_val, axis=-1).numpy()\n",
        "        val_accuracy = np.mean(val_preds == true_val)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "              f\"Training Accuracy: {train_accuracy:.4f} | Val Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the Model (Test Loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, X_test, Y_test, sequence_length, training_length=100):\n",
        "    X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "    Y_test = tf.convert_to_tensor(Y_test, dtype=tf.float32)\n",
        "    \n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    \n",
        "    test_loss = 0\n",
        "    hidden_state = None\n",
        "    num_test_segments = sequence_length // training_length\n",
        "\n",
        "    all_test_preds = []  # Collect all predictions for final accuracy\n",
        "    all_true_test = []   # Collect all true labels for final accuracy\n",
        "\n",
        "    for i in range(num_test_segments):\n",
        "        start = i * training_length\n",
        "        end = min((i + 1) * training_length, sequence_length)\n",
        "        \n",
        "        X_segment = X_test[:, start:end, :]\n",
        "        Y_segment = tf.argmax(Y_test[:, start:end, :], axis=-1)\n",
        "\n",
        "        if hidden_state is None:\n",
        "            output, hidden_state = model(X_segment)  # First segment\n",
        "        else:\n",
        "            output, hidden_state = model(X_segment, initial_state=hidden_state)  # Keep state\n",
        "\n",
        "        segment_loss = loss_fn(Y_segment, output).numpy()\n",
        "        test_loss += segment_loss\n",
        "\n",
        "        test_preds = np.argmax(output.numpy(), axis=-1)\n",
        "        true_test = Y_segment.numpy()\n",
        "\n",
        "        # Collect for final accuracy calculation\n",
        "        all_test_preds.append(test_preds)\n",
        "        all_true_test.append(true_test)\n",
        "        \n",
        "        if num_test_segments > 1:\n",
        "            segment_accuracy = np.mean(test_preds == true_test)\n",
        "            print(f\"Segment {i + 1}/{num_test_segments} | Loss: {segment_loss:.4f} | Accuracy: {segment_accuracy:.4f}\")\n",
        "\n",
        "    # Final test loss\n",
        "    test_loss /= num_test_segments\n",
        "\n",
        "    # Combine all predictions and labels for final accuracy\n",
        "    all_test_preds = np.concatenate(all_test_preds, axis=1)\n",
        "    all_true_test = np.concatenate(all_true_test, axis=1)\n",
        "    test_accuracy = np.mean(all_test_preds == all_true_test)\n",
        "    \n",
        "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Benchmarking all Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnUjvLTtUys",
        "outputId": "0d895fad-60ab-4637-cf95-2df4bc18466c"
      },
      "outputs": [],
      "source": [
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    vocabulary = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    delimiter = 10\n",
        "    \n",
        "    # Hyper-parameters used across each model\n",
        "    sequence_lengths = [100, 200, 500, 1000]\n",
        "    sequence_length = sequence_lengths[1]\n",
        "    sequence_count = 100\n",
        "    total_delimiters = 3 # Adds a few delimiters to make it clear that its the end of the sequence.\n",
        "    input_size = len(vocabulary) + total_delimiters # Vocabulary tokens + delimiter token\n",
        "    hidden_size = 128\n",
        "    training_length = 100\n",
        "    total_epochs = 5\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.01\n",
        "    \n",
        "    X_train, X_val, X_test, Y_train, Y_val, Y_test = generate_dataset_splits(sequence_count, input_size, training_length, sequence_length, delimiter, total_delimiters)\n",
        "    \n",
        "    models = {\n",
        "        \"Standard LSTM\": StandardLSTM(input_size, hidden_size),\n",
        "        \"Multiplicative LSTM\": MultiplicativeLSTM(input_size, hidden_size),\n",
        "        \"Standard GRU\": StandardGRU(input_size, hidden_size),\n",
        "        \"Multiplicative GRU\": MultiplicativeGRU(input_size, hidden_size)\n",
        "    }\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\\n\")\n",
        "        train_model(model, X_train, Y_train, X_val, Y_val, total_epochs, batch_size, learning_rate)\n",
        "        print(f'\\nTesting {name}...\\n')\n",
        "        test_model(model, X_test, Y_test, sequence_length, training_length)\n",
        "        print(\"=======================================================================================================\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Standard LSTM...\n",
            "\n",
            "Epoch 01 | Training Loss: 2.4130 | Val Loss: 2.2149 | Training Accuracy: 0.2015 | Val Accuracy: 0.3774\n",
            "Epoch 02 | Training Loss: 2.0699 | Val Loss: 1.8415 | Training Accuracy: 0.4684 | Val Accuracy: 0.4499\n",
            "Epoch 03 | Training Loss: 1.7144 | Val Loss: 1.5069 | Training Accuracy: 0.5704 | Val Accuracy: 0.5714\n",
            "Epoch 04 | Training Loss: 1.3861 | Val Loss: 1.2453 | Training Accuracy: 0.6286 | Val Accuracy: 0.6388\n",
            "Epoch 05 | Training Loss: 1.1484 | Val Loss: 1.0717 | Training Accuracy: 0.7015 | Val Accuracy: 0.6760\n",
            "\n",
            "Testing Standard LSTM...\n",
            "\n",
            "Segment 1/2 | Loss: 1.1164 | Accuracy: 0.6534\n",
            "Segment 2/2 | Loss: 1.1015 | Accuracy: 0.6650\n",
            "\n",
            "Test Loss: 1.1090 | Test Accuracy: 0.6592\n",
            "Training Multiplicative LSTM...\n",
            "\n",
            "Epoch 01 | Training Loss: 2.4191 | Val Loss: 2.2694 | Training Accuracy: 0.3228 | Val Accuracy: 0.2249\n",
            "Epoch 02 | Training Loss: 2.1094 | Val Loss: 1.8909 | Training Accuracy: 0.4442 | Val Accuracy: 0.4253\n",
            "Epoch 03 | Training Loss: 1.7512 | Val Loss: 1.5586 | Training Accuracy: 0.5243 | Val Accuracy: 0.5127\n",
            "Epoch 04 | Training Loss: 1.4620 | Val Loss: 1.3362 | Training Accuracy: 0.5388 | Val Accuracy: 0.5667\n",
            "Epoch 05 | Training Loss: 1.2405 | Val Loss: 1.1873 | Training Accuracy: 0.6529 | Val Accuracy: 0.6069\n",
            "\n",
            "Testing Multiplicative LSTM...\n",
            "\n",
            "Segment 1/2 | Loss: 1.2404 | Accuracy: 0.5861\n",
            "Segment 2/2 | Loss: 1.2141 | Accuracy: 0.5938\n",
            "\n",
            "Test Loss: 1.2272 | Test Accuracy: 0.5899\n",
            "Training Standard GRU...\n",
            "\n",
            "Epoch 01 | Training Loss: 2.4786 | Val Loss: 2.2066 | Training Accuracy: 0.2403 | Val Accuracy: 0.2552\n",
            "Epoch 02 | Training Loss: 2.0722 | Val Loss: 1.8412 | Training Accuracy: 0.4102 | Val Accuracy: 0.5105\n",
            "Epoch 03 | Training Loss: 1.7099 | Val Loss: 1.5258 | Training Accuracy: 0.5558 | Val Accuracy: 0.5363\n",
            "Epoch 04 | Training Loss: 1.4073 | Val Loss: 1.2477 | Training Accuracy: 0.6044 | Val Accuracy: 0.6471\n",
            "Epoch 05 | Training Loss: 1.1563 | Val Loss: 1.0700 | Training Accuracy: 0.6408 | Val Accuracy: 0.6811\n",
            "\n",
            "Testing Standard GRU...\n",
            "\n",
            "Segment 1/2 | Loss: 1.1071 | Accuracy: 0.6613\n",
            "Segment 2/2 | Loss: 1.0830 | Accuracy: 0.6766\n",
            "\n",
            "Test Loss: 1.0950 | Test Accuracy: 0.6690\n",
            "Training Multiplicative GRU...\n",
            "\n",
            "Epoch 01 | Training Loss: 2.4324 | Val Loss: 2.2496 | Training Accuracy: 0.2451 | Val Accuracy: 0.2625\n",
            "Epoch 02 | Training Loss: 2.1113 | Val Loss: 1.9467 | Training Accuracy: 0.3544 | Val Accuracy: 0.3554\n",
            "Epoch 03 | Training Loss: 1.8261 | Val Loss: 1.6992 | Training Accuracy: 0.4903 | Val Accuracy: 0.4370\n",
            "Epoch 04 | Training Loss: 1.5840 | Val Loss: 1.4975 | Training Accuracy: 0.5388 | Val Accuracy: 0.5038\n",
            "Epoch 05 | Training Loss: 1.3785 | Val Loss: 1.2909 | Training Accuracy: 0.5583 | Val Accuracy: 0.5593\n",
            "\n",
            "Testing Multiplicative GRU...\n",
            "\n",
            "Segment 1/2 | Loss: 1.3382 | Accuracy: 0.5423\n",
            "Segment 2/2 | Loss: 1.3295 | Accuracy: 0.5534\n",
            "\n",
            "Test Loss: 1.3339 | Test Accuracy: 0.5478\n"
          ]
        }
      ],
      "source": [
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TODO\n",
        "\n",
        "- Add accuracy measurement\n",
        "- Look into vanishing/exploding gradients and how these can be displayed (need this for analysis later)\n",
        "- Do copy task, figure out what a delimitor is\n",
        "- Remember, 3 copy tasks need to be ran on each model so that mean accuracy and standard error can be measured\n",
        "- look into cross entropy??"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
