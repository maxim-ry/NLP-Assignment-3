{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 3: Intro to Recurrent Neural Networks: Math, Training, and the Copy Task\n",
        "\n",
        "# Instructor: Dr. Ankur Mali\n",
        "# University of South Florida (Spring 2025)\n",
        "### In this tutorial we will build RNNs based on equation and will compare 3 popular frameworks (Jax, TensorFlow and Pytorch)\n",
        "\n",
        "## Vanilla RNN -- For more in depth explanation refer to your slides\n",
        "\n",
        "### Forward Pass (Inference) -- Stage 1\n",
        "Given an input at time \\(t\\):\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}_t \\in \\mathbb{R}^{d_{\\text{in}}},\\quad \\mathbf{h}_{t-1} \\in \\mathbb{R}^{d_{\\text{hid}}}\n",
        "\\end{aligned}\n",
        "we define RNN parameters:\n",
        "\\begin{aligned}\n",
        "\\mathbf{W}_x \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{hid}}}, \\quad\n",
        "\\mathbf{W}_h \\in \\mathbb{R}^{d_{\\text{hid}} \\times d_{\\text{hid}}}, \\quad\n",
        "\\mathbf{b}_h \\in \\mathbb{R}^{d_{\\text{hid}}}.\n",
        "\\end{aligned}\n",
        "\n",
        "The hidden state update:\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_t = \\tanh\\Bigl(\\mathbf{x}_t\\,\\mathbf{W}_x \\;+\\;\\mathbf{h}_{t-1}\\,\\mathbf{W}_h \\;+\\;\\mathbf{b}_h\\Bigr).\n",
        "\\end{aligned}\n",
        "\n",
        "Over a sequence  ($\\mathbf{x}_1$, $\\dots$, $\\mathbf{x}_T$), we unroll:\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_0 = \\mathbf{0},\\quad\n",
        "\\mathbf{h}_1 = \\tanh(\\mathbf{x}_1 \\mathbf{W}_x + \\mathbf{h}_0 \\mathbf{W}_h + \\mathbf{b}_h),\\,\\dots,\\,\n",
        "\\mathbf{h}_T = \\tanh(\\mathbf{x}_T \\mathbf{W}_x + \\mathbf{h}_{T-1} \\mathbf{W}_h + \\mathbf{b}_h).\n",
        "\\end{aligned}\n",
        "\n",
        "Optionally, each hidden state  \\($\\mathbf{h}_t$\\) can be projected to the output dimension $d_{\\text{in}}$:\n",
        "\\begin{aligned}\n",
        "\\mathbf{\\hat{y}}_t = \\mathbf{h}_t \\mathbf{W}_{\\text{out}} + \\mathbf{b}_{\\text{out}}\n",
        "\\end{aligned}\n",
        "\n",
        "<!-- $\\mathbf{\\hat{y}}$_t = $\\mathbf{h}_t$,$\\mathbf{W}_{\\text{out}}$ + $\\mathbf{b}_{\\text{out}}$. -->\n",
        "\n",
        "\n",
        "### Remaining Stages\n",
        "We define a loss (Stage 2) over all time steps, for instance:\n",
        "\\begin{aligned}\n",
        "\\mathbf{L} = \\frac{1}{T} \\sum_{t=1}^T \\left\\|\\,\\mathbf{\\hat{y}}_t - \\mathbf{y}_t\\,\\right\\|^2,\n",
        "\\end{aligned}\n",
        "and use Backpropagation Through Time (BPTT) (Stage 3). An optimizer (e.g., Adam) updates parameters (Stage 4):\n",
        "\\begin{aligned}\n",
        "\\theta \\,\\leftarrow\\, \\theta \\;-\\; \\eta \\,\\nabla_\\theta \\,\\mathbf{L}.\n",
        "\\end{aligned}\n",
        "\n",
        "---\n",
        "\n",
        "## GRU\n",
        "\n",
        "### Forward Pass (Inference)\n",
        "A Gated Recurrent Unit includes reset $\\mathbf{r}_t$ and update $\\mathbf{z}_t$ gates:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_t &= \\sigma\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_z + \\mathbf{h}_{t-1}\\,\\mathbf{U}_z + \\mathbf{b}_z\\bigr), \\\\\n",
        "\\mathbf{r}_t &= \\sigma\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_r + \\mathbf{h}_{t-1}\\,\\mathbf{U}_r + \\mathbf{b}_r\\bigr), \\\\\n",
        "\\tilde{\\mathbf{h}}_t &= \\tanh\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_h + (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1})\\,\\mathbf{U}_h + \\mathbf{b}_h\\bigr), \\\\\n",
        "\\mathbf{h}_t &= (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} \\;+\\; \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t.\n",
        "\\end{aligned}\n",
        "\n",
        "where $\\sigma$ is the sigmoid function, and $\\odot$ denotes elementwise multiplication.\n",
        "\n",
        "### Remaining Stages\n",
        "As in the vanilla RNN, define a loss $\\mathbf{L}$ (e.g. MSE). The same BPTT logic applies, but the derivatives now include the GRU gating operations. Parameters (e.g., $\\mathbf{W}_z, \\mathbf{U}_z, \\ldots$ ) are updated by any gradient-based optimizer.\n",
        "\n",
        "---\n",
        "\n",
        "## Optimizer\n",
        "A typical training loop includes:\n",
        "\n",
        "1. **Forward pass**: compute model outputs $\\mathbf{\\hat{y}}_t$.\n",
        "2. **Loss computation**: $\\mathbf{L}(\\mathbf{\\hat{y}}_t, \\mathbf{y}_t)$.\n",
        "3. **Backward pass**: compute $\\nabla_\\theta \\mathbf{L}$ via BPTT.\n",
        "4. **Parameter update**:\n",
        "   \\begin{aligned}\n",
        "   \\theta \\leftarrow \\theta - \\eta \\;\\nabla_\\theta \\,\\mathcal{L}.\n",
        "   \\end{aligned}\n",
        "   (For example, using Adam, SGD, RMSProp, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "## The Copy Task\n",
        "The **copy task** is a simple sequence-to-sequence challenge:\n",
        "\n",
        "- **Input**: a sequence of random vectors {$\\mathbf{x}_1, \\dots, \\mathbf{x}_T$}.\n",
        "- **Target**: the **same** sequence {$\\mathbf{x}_1, \\dots, \\mathbf{x}_T$}.\n",
        "\n",
        "Thus, the model should learn to produce $\\mathbf{\\hat{y}}_t \\approx \\mathbf{x}_t$ at each time step ($t$). It's a straightforward yet revealing test of a model’s capacity to retain and reproduce a sequence—particularly sensitive to the model’s ability to **remember** information over time.  \n"
      ],
      "metadata": {
        "id": "PuInr3Sk0J3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c6lcugDetLTx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, lax\n",
        "import time\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "########################################\n",
        "# Custom RNN Cell (Core Computation)\n",
        "########################################\n",
        "\n",
        "# ------- PyTorch Single-Step RNN Cell -------\n",
        "class RNNCellPyTorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in PyTorch.\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # For a single step:  input: (batch_size, input_size)\n",
        "        #                    hidden: (batch_size, hidden_size)\n",
        "        self.W_x = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.W_h = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_h = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        # x_t: [batch_size, input_size]\n",
        "        # h_prev: [batch_size, hidden_size]\n",
        "        h_t = torch.tanh(x_t @ self.W_x + h_prev @ self.W_h + self.b_h)\n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level PyTorch RNN that unrolls over time -------\n",
        "class RNNPyTorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Unrolls the RNNCell over a full sequence.\n",
        "    Also includes an output projection from hidden_size -> input_size\n",
        "    so we can do an MSE loss vs. the original input.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = RNNCellPyTorch(input_size, hidden_size)\n",
        "        # Output projection to match the original input dimension for copy task\n",
        "        self.W_out = torch.nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
        "        self.b_out = torch.nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size, seq_length, _ = X.shape\n",
        "        h = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]  # [batch_size, input_size]\n",
        "            h = self.rnn_cell(x_t, h)  # [batch_size, hidden_size]\n",
        "            # Project hidden -> input_size\n",
        "            out_t = h @ self.W_out + self.b_out\n",
        "            outputs.append(out_t.unsqueeze(1))  # shape [batch_size,1,input_size]\n",
        "        # Concatenate across time\n",
        "        return torch.cat(outputs, dim=1)  # [batch_size, seq_length, input_size]\n",
        "\n",
        "########################################\n",
        "# TensorFlow Implementation\n",
        "########################################\n",
        "\n",
        "# ------- Single-Step RNN Cell -------\n",
        "class RNNCellTF(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in TensorFlow.\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_x = self.add_weight(\n",
        "            shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.W_h = self.add_weight(\n",
        "            shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b_h = self.add_weight(\n",
        "            shape=(hidden_size,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        h_t = tf.math.tanh(\n",
        "            tf.matmul(x_t, self.W_x) + tf.matmul(h_prev, self.W_h) + self.b_h\n",
        "        )\n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class RNNTF(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = RNNCellTF(input_size, hidden_size)\n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(\n",
        "            shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b_out = self.add_weight(\n",
        "            shape=(input_size,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.rnn_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "# Training / Benchmark\n",
        "########################################\n",
        "\n",
        "# -------------- PyTorch Benchmark --------------\n",
        "def benchmark_pytorch(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    model = RNNPyTorch(input_size, hidden_size)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    start_time = time.time()\n",
        "\n",
        "    X_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_torch = torch.tensor(Y_train, dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_torch)  # [batch_size, seq_length, input_size]\n",
        "        loss = criterion(output, Y_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(f\"Epoch {epoch} | Loss torch: {loss.item():.6f}\")\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "# -------------- TensorFlow Benchmark --------------\n",
        "def benchmark_tensorflow(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    model = RNNTF(input_size, hidden_size)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    X_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y_tf = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(X_tf)\n",
        "            loss = loss_fn(output, Y_tf)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        #print(f\"Epoch {epoch} | Loss TF: {loss.numpy():.6f}\")\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "\n",
        "########################################\n",
        "# JAX Implementation -- Faster\n",
        "########################################\n",
        "\n",
        "def RNNCellJAX(params, x_t, h_prev):\n",
        "    W_x, W_h, b_h = params\n",
        "    h_t = jnp.tanh(jnp.dot(x_t, W_x) + jnp.dot(h_prev, W_h) + b_h)\n",
        "    return h_t\n",
        "\n",
        "def RNNJAX_unroll(params, X):\n",
        "    W_x, W_h, b_h, W_out, b_out = params\n",
        "    batch_size, seq_length, _ = X.shape\n",
        "    X_t = jnp.swapaxes(X, 0, 1)  # [seq_length, batch_size, input_size]\n",
        "\n",
        "    def step_fn(h_prev, x_t):\n",
        "        h_t = jnp.tanh(\n",
        "            jnp.dot(x_t, W_x)\n",
        "            + jnp.dot(h_prev, W_h)\n",
        "            + b_h\n",
        "        )\n",
        "        out_t = jnp.dot(h_t, W_out) + b_out\n",
        "        return h_t, out_t\n",
        "\n",
        "    h0 = jnp.zeros((batch_size, W_h.shape[0]))\n",
        "    final_h, outs = lax.scan(step_fn, h0, X_t)\n",
        "    # outs: [seq_length, batch_size, input_size]\n",
        "    outs = jnp.swapaxes(outs, 0, 1)  # [batch_size, seq_length, input_size]\n",
        "    return outs\n",
        "\n",
        "########################################\n",
        "# Training / Benchmark\n",
        "########################################\n",
        "\n",
        "def init_jax_params(key, input_size, hidden_size):\n",
        "    k1, k2, k3, k4, k5 = jax.random.split(key, 5)\n",
        "    W_x = 0.1 * jax.random.normal(k1, (input_size, hidden_size))\n",
        "    W_h = 0.1 * jax.random.normal(k2, (hidden_size, hidden_size))\n",
        "    b_h = jnp.zeros((hidden_size,))\n",
        "    W_out = 0.1 * jax.random.normal(k3, (hidden_size,  input_size))\n",
        "    b_out = jnp.zeros((input_size,))\n",
        "    return (W_x, W_h, b_h, W_out, b_out)\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    pred = RNNJAX_unroll(params, x)\n",
        "    return jnp.mean((pred - y) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, x, y, lr):\n",
        "    grads = jax.grad(loss_fn)(params, x, y)\n",
        "    new_params = []\n",
        "    for p, g in zip(params, grads):\n",
        "        new_params.append(p - lr * g)\n",
        "    return tuple(new_params)\n",
        "\n",
        "\n",
        "def benchmark_jax(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    key = random.PRNGKey(42)\n",
        "    params = init_jax_params(key, input_size, hidden_size)\n",
        "\n",
        "    X_jax = jnp.array(X_train)\n",
        "    Y_jax = jnp.array(Y_train)\n",
        "\n",
        "    # warm-up to compile\n",
        "    _ = train_step(params, X_jax, Y_jax, lr)\n",
        "\n",
        "    start_time = time.time()\n",
        "    p = params\n",
        "    for epoch in range(epochs):\n",
        "        p = train_step(p, X_jax, Y_jax, lr)\n",
        "        #current_loss = loss_fn(p, X_jax, Y_jax)\n",
        "        #print(f\"Epoch {epoch} | Loss jax: {current_loss:.6f}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return total_time\n",
        "\n",
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    seq_length = 20\n",
        "    batch_size = 32\n",
        "    input_size = 10\n",
        "    hidden_size = 128\n",
        "    num_epochs = 10\n",
        "\n",
        "    np.random.seed(42)\n",
        "    X_train = np.random.rand(1000, seq_length, input_size).astype(np.float32)\n",
        "    Y_train = X_train.copy()\n",
        "\n",
        "    # PyTorch\n",
        "    pytorch_time = benchmark_pytorch(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "    # TensorFlow\n",
        "    tensorflow_time = benchmark_tensorflow(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "    # JAX\n",
        "    jax_time = benchmark_jax(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "    print(f\"PyTorch Time: {pytorch_time:.4f} s\")\n",
        "    print(f\"TensorFlow Time: {tensorflow_time:.4f} s\")\n",
        "    print(f\"JAX Time: {jax_time:.4f} s\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Things to Learn\n",
        "\n",
        "########################################\n",
        "# JAX Implementation -- slow version -- This will work (Learn how to speedup things in JAX by comparing two implementation)\n",
        "########################################\n",
        "# def RNNCellJAX(params, x_t, h_prev):\n",
        "#     \"\"\"\n",
        "#     Single-step RNN cell in JAX.\n",
        "#     params = (W_x, W_h, b_h)\n",
        "#     h_t = tanh( x_t*W_x + h_prev*W_h + b )\n",
        "#     \"\"\"\n",
        "#     W_x, W_h, b_h = params\n",
        "#     h_t = jnp.tanh(jnp.dot(x_t, W_x) + jnp.dot(h_prev, W_h) + b_h)\n",
        "#     return h_t\n",
        "\n",
        "# def RNNJAX_unroll(params, X):\n",
        "#     \"\"\"\n",
        "#     Unroll RNNCellJAX across time.\n",
        "#     params_main = (W_x, W_h, b_h, W_out, b_out)\n",
        "#     X: [batch_size, seq_length, input_size]\n",
        "#     We'll swap so we scan over seq_length dimension.\n",
        "#     \"\"\"\n",
        "#     W_x, W_h, b_h, W_out, b_out = params\n",
        "#     batch_size, seq_length, _ = X.shape\n",
        "\n",
        "#     # Swap to [seq_length, batch_size, input_size]\n",
        "#     X_t = jnp.swapaxes(X, 0, 1)\n",
        "\n",
        "#     def step_fn(h_prev, x_t):\n",
        "#         h_t = jnp.tanh(jnp.dot(x_t, W_x) + jnp.dot(h_prev, W_h) + b_h)\n",
        "#         # Output projection back to input_size\n",
        "#         out_t = jnp.dot(h_t, W_out) + b_out\n",
        "#         return h_t, out_t\n",
        "\n",
        "#     h0 = jnp.zeros((batch_size, W_h.shape[0]))\n",
        "#     final_h, outs = lax.scan(step_fn, h0, X_t)\n",
        "#     # outs: [seq_length, batch_size, input_size]\n",
        "#     # we want [batch_size, seq_length, input_size], so swap axes\n",
        "#     outs = jnp.swapaxes(outs, 0, 1)\n",
        "#     return outs\n",
        "\n",
        "\n",
        "# -------------- JAX Benchmark -- slow version--------------\n",
        "# def init_jax_params(key, input_size, hidden_size):\n",
        "#     # W_x: [input_size, hidden_size]\n",
        "#     # W_h: [hidden_size, hidden_size]\n",
        "#     # b_h: [hidden_size]\n",
        "#     # W_out: [hidden_size, input_size]\n",
        "#     # b_out: [input_size]\n",
        "\n",
        "#     k1, k2, k3, k4, k5 = jax.random.split(key, 5)\n",
        "#     W_x = 0.1 * jax.random.normal(k1, (input_size, hidden_size))\n",
        "#     W_h = 0.1 * jax.random.normal(k2, (hidden_size, hidden_size))\n",
        "#     b_h = jnp.zeros((hidden_size,))\n",
        "#     W_out = 0.1 * jax.random.normal(k3, (hidden_size,  input_size))\n",
        "#     b_out = jnp.zeros((input_size,))\n",
        "#     return (W_x, W_h, b_h, W_out, b_out)\n",
        "\n",
        "# def benchmark_jax(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "#     key = random.PRNGKey(42)\n",
        "#     params = init_jax_params(key, input_size, hidden_size)\n",
        "\n",
        "#     def loss_fn(p, x, y):\n",
        "#         pred = RNNJAX_unroll(p, x)\n",
        "#         return jnp.mean((pred - y) ** 2)\n",
        "\n",
        "#     grad_fn = jax.grad(loss_fn)\n",
        "#     X_jax = jnp.array(X_train)\n",
        "#     Y_jax = jnp.array(Y_train)\n",
        "\n",
        "#     start_time = time.time()\n",
        "#     p = params\n",
        "#     for epoch in range(epochs):\n",
        "#         grads = grad_fn(p, X_jax, Y_jax)\n",
        "#         p = [param - lr*g for param, g in zip(p, grads)]\n",
        "\n",
        "#     return time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnUjvLTtUys",
        "outputId": "0d895fad-60ab-4637-cf95-2df4bc18466c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Time: 0.2036 s\n",
            "TensorFlow Time: 1.1847 s\n",
            "JAX Time: 0.0039 s\n"
          ]
        }
      ]
    }
  ]
}