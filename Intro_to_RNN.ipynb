{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuInr3Sk0J3C"
      },
      "source": [
        "# Tutorial 3: Intro to Recurrent Neural Networks: Math, Training, and the Copy Task\n",
        "\n",
        "# Instructor: Dr. Ankur Mali\n",
        "# University of South Florida (Spring 2025)\n",
        "### In this tutorial we will build RNNs based on equation and will compare 3 popular frameworks (Jax, TensorFlow and Pytorch)\n",
        "\n",
        "## Vanilla RNN -- For more in depth explanation refer to your slides\n",
        "\n",
        "### Forward Pass (Inference) -- Stage 1\n",
        "Given an input at time \\(t\\):\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}_t \\in \\mathbb{R}^{d_{\\text{in}}},\\quad \\mathbf{h}_{t-1} \\in \\mathbb{R}^{d_{\\text{hid}}}\n",
        "\\end{aligned}\n",
        "we define RNN parameters:\n",
        "\\begin{aligned}\n",
        "\\mathbf{W}_x \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{hid}}}, \\quad\n",
        "\\mathbf{W}_h \\in \\mathbb{R}^{d_{\\text{hid}} \\times d_{\\text{hid}}}, \\quad\n",
        "\\mathbf{b}_h \\in \\mathbb{R}^{d_{\\text{hid}}}.\n",
        "\\end{aligned}\n",
        "\n",
        "The hidden state update:\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_t = \\tanh\\Bigl(\\mathbf{x}_t\\,\\mathbf{W}_x \\;+\\;\\mathbf{h}_{t-1}\\,\\mathbf{W}_h \\;+\\;\\mathbf{b}_h\\Bigr).\n",
        "\\end{aligned}\n",
        "\n",
        "Over a sequence  ($\\mathbf{x}_1$, $\\dots$, $\\mathbf{x}_T$), we unroll:\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_0 = \\mathbf{0},\\quad\n",
        "\\mathbf{h}_1 = \\tanh(\\mathbf{x}_1 \\mathbf{W}_x + \\mathbf{h}_0 \\mathbf{W}_h + \\mathbf{b}_h),\\,\\dots,\\,\n",
        "\\mathbf{h}_T = \\tanh(\\mathbf{x}_T \\mathbf{W}_x + \\mathbf{h}_{T-1} \\mathbf{W}_h + \\mathbf{b}_h).\n",
        "\\end{aligned}\n",
        "\n",
        "Optionally, each hidden state  \\($\\mathbf{h}_t$\\) can be projected to the output dimension $d_{\\text{in}}$:\n",
        "\\begin{aligned}\n",
        "\\mathbf{\\hat{y}}_t = \\mathbf{h}_t \\mathbf{W}_{\\text{out}} + \\mathbf{b}_{\\text{out}}\n",
        "\\end{aligned}\n",
        "\n",
        "<!-- $\\mathbf{\\hat{y}}$_t = $\\mathbf{h}_t$,$\\mathbf{W}_{\\text{out}}$ + $\\mathbf{b}_{\\text{out}}$. -->\n",
        "\n",
        "\n",
        "### Remaining Stages\n",
        "We define a loss (Stage 2) over all time steps, for instance:\n",
        "\\begin{aligned}\n",
        "\\mathbf{L} = \\frac{1}{T} \\sum_{t=1}^T \\left\\|\\,\\mathbf{\\hat{y}}_t - \\mathbf{y}_t\\,\\right\\|^2,\n",
        "\\end{aligned}\n",
        "and use Backpropagation Through Time (BPTT) (Stage 3). An optimizer (e.g., Adam) updates parameters (Stage 4):\n",
        "\\begin{aligned}\n",
        "\\theta \\,\\leftarrow\\, \\theta \\;-\\; \\eta \\,\\nabla_\\theta \\,\\mathbf{L}.\n",
        "\\end{aligned}\n",
        "\n",
        "---\n",
        "\n",
        "## GRU\n",
        "\n",
        "### Forward Pass (Inference)\n",
        "A Gated Recurrent Unit includes reset $\\mathbf{r}_t$ and update $\\mathbf{z}_t$ gates:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_t &= \\sigma\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_z + \\mathbf{h}_{t-1}\\,\\mathbf{U}_z + \\mathbf{b}_z\\bigr), \\\\\n",
        "\\mathbf{r}_t &= \\sigma\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_r + \\mathbf{h}_{t-1}\\,\\mathbf{U}_r + \\mathbf{b}_r\\bigr), \\\\\n",
        "\\tilde{\\mathbf{h}}_t &= \\tanh\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_h + (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1})\\,\\mathbf{U}_h + \\mathbf{b}_h\\bigr), \\\\\n",
        "\\mathbf{h}_t &= (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} \\;+\\; \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t.\n",
        "\\end{aligned}\n",
        "\n",
        "where $\\sigma$ is the sigmoid function, and $\\odot$ denotes elementwise multiplication.\n",
        "\n",
        "### Remaining Stages\n",
        "As in the vanilla RNN, define a loss $\\mathbf{L}$ (e.g. MSE). The same BPTT logic applies, but the derivatives now include the GRU gating operations. Parameters (e.g., $\\mathbf{W}_z, \\mathbf{U}_z, \\ldots$ ) are updated by any gradient-based optimizer.\n",
        "\n",
        "---\n",
        "\n",
        "## Optimizer\n",
        "A typical training loop includes:\n",
        "\n",
        "1. **Forward pass**: compute model outputs $\\mathbf{\\hat{y}}_t$.\n",
        "2. **Loss computation**: $\\mathbf{L}(\\mathbf{\\hat{y}}_t, \\mathbf{y}_t)$.\n",
        "3. **Backward pass**: compute $\\nabla_\\theta \\mathbf{L}$ via BPTT.\n",
        "4. **Parameter update**:\n",
        "   \\begin{aligned}\n",
        "   \\theta \\leftarrow \\theta - \\eta \\;\\nabla_\\theta \\,\\mathcal{L}.\n",
        "   \\end{aligned}\n",
        "   (For example, using Adam, SGD, RMSProp, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "## The Copy Task\n",
        "The **copy task** is a simple sequence-to-sequence challenge:\n",
        "\n",
        "- **Input**: a sequence of random vectors {$\\mathbf{x}_1, \\dots, \\mathbf{x}_T$}.\n",
        "- **Target**: the **same** sequence {$\\mathbf{x}_1, \\dots, \\mathbf{x}_T$}.\n",
        "\n",
        "Thus, the model should learn to produce $\\mathbf{\\hat{y}}_t \\approx \\mathbf{x}_t$ at each time step ($t$). It's a straightforward yet revealing test of a model’s capacity to retain and reproduce a sequence—particularly sensitive to the model’s ability to **remember** information over time.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c6lcugDetLTx"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# Custom RNN Cell (Core Computation)\n",
        "########################################\n",
        "\n",
        "########################################\n",
        "# TensorFlow Implementation\n",
        "########################################\n",
        "\n",
        "# ------- Single-Step RNN Cell -------\n",
        "class RNNCellTF(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in TensorFlow.\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_x = self.add_weight(\n",
        "            shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.W_h = self.add_weight(\n",
        "            shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b_h = self.add_weight(\n",
        "            shape=(hidden_size,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        h_t = tf.math.tanh(\n",
        "            tf.matmul(x_t, self.W_x) + tf.matmul(h_prev, self.W_h) + self.b_h\n",
        "        )\n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class RNNTF(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = RNNCellTF(input_size, hidden_size)\n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(\n",
        "            shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b_out = self.add_weight(\n",
        "            shape=(input_size,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.rnn_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "# Training / Benchmark\n",
        "########################################\n",
        "\n",
        "# -------------- TensorFlow Benchmark --------------\n",
        "def benchmark_tensorflow(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    model = RNNTF(input_size, hidden_size)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    X_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y_tf = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(X_tf)\n",
        "            loss = loss_fn(output, Y_tf)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        #print(f\"Epoch {epoch} | Loss TF: {loss.numpy():.6f}\")\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "\n",
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    seq_length = 20\n",
        "    batch_size = 32\n",
        "    input_size = 10\n",
        "    hidden_size = 128\n",
        "    num_epochs = 10\n",
        "\n",
        "    np.random.seed(42)\n",
        "    X_train = np.random.rand(1000, seq_length, input_size).astype(np.float32)\n",
        "    Y_train = X_train.copy()\n",
        "\n",
        "    # TensorFlow\n",
        "    tensorflow_time = benchmark_tensorflow(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "\n",
        "    print(f\"TensorFlow Time: {tensorflow_time:.4f} s\")\n",
        "\n",
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Standard LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        i_t = tf.sigmoid(tf.matmul(x_t, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_t, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_t, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_t, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = StandardLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiplicative LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Learnable memory weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Input gate weights and bias\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights and bias\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights and bias\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights and bias\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        # Multiplicative Extension\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        i_t = tf.sigmoid(tf.matmul(x_cap, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_cap, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_cap, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_cap, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "class MultiplicativeLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = MultiplicativeLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnUjvLTtUys",
        "outputId": "0d895fad-60ab-4637-cf95-2df4bc18466c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-13 14:09:51.342522: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [1000,128] vs. [1000,10]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling MultiplicativeLSTMCell.call().\n\n\u001b[1m{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1000,128] vs. [1000,10] [Op:Mul] name: \u001b[0m\n\nArguments received by MultiplicativeLSTMCell.call():\n  • x_t=tf.Tensor(shape=(1000, 10), dtype=float32)\n  • h_prev=tf.Tensor(shape=(1000, 128), dtype=float32)\n  • c_prev=tf.Tensor(shape=(1000, 128), dtype=float32)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 45\u001b[0m\n\u001b[1;32m     40\u001b[0m     tensorflow_time \u001b[38;5;241m=\u001b[39m benchmark_rnn_models(input_size, hidden_size, X_train, Y_train, num_epochs)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensorflow_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[16], line 40\u001b[0m, in \u001b[0;36mrun_benchmark\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# TensorFlow\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m tensorflow_time \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_rnn_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensorflow_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mbenchmark_rnn_models\u001b[0;34m(input_size, hidden_size, X_train, Y_train, epochs, lr)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 16\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(output, Y)\n\u001b[1;32m     18\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
            "File \u001b[0;32m~/USF-Spring-2025/CAP4641/NLP-Assignment-3/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "Cell \u001b[0;32mIn[15], line 67\u001b[0m, in \u001b[0;36mMultiplicativeLSTM.call\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_length):\n\u001b[1;32m     66\u001b[0m     x_t \u001b[38;5;241m=\u001b[39m X[:, t, :]\n\u001b[0;32m---> 67\u001b[0m     h, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     out_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_out) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_out\n\u001b[1;32m     69\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mexpand_dims(out_t, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
            "Cell \u001b[0;32mIn[15], line 34\u001b[0m, in \u001b[0;36mMultiplicativeLSTMCell.call\u001b[0;34m(self, x_t, h_prev, c_prev)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_t, h_prev, c_prev):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Multiplicative Extension\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     m_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(x_t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_m) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(h_prev, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_m\n\u001b[0;32m---> 34\u001b[0m     x_cap \u001b[38;5;241m=\u001b[39m \u001b[43mm_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\n\u001b[1;32m     36\u001b[0m     i_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msigmoid(tf\u001b[38;5;241m.\u001b[39mmatmul(x_cap, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_i) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(h_prev, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_i)\n\u001b[1;32m     37\u001b[0m     f_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msigmoid(tf\u001b[38;5;241m.\u001b[39mmatmul(x_cap, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_f) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(h_prev, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_f) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_f)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling MultiplicativeLSTMCell.call().\n\n\u001b[1m{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1000,128] vs. [1000,10] [Op:Mul] name: \u001b[0m\n\nArguments received by MultiplicativeLSTMCell.call():\n  • x_t=tf.Tensor(shape=(1000, 10), dtype=float32)\n  • h_prev=tf.Tensor(shape=(1000, 128), dtype=float32)\n  • c_prev=tf.Tensor(shape=(1000, 128), dtype=float32)"
          ]
        }
      ],
      "source": [
        "def benchmark_rnn_models(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    model = RNNTF(input_size, hidden_size)\n",
        "\n",
        "    X = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "\n",
        "    # Standard LSTM Benchmark\n",
        "    model = StandardLSTM(input_size, hidden_size)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(X)\n",
        "            loss = loss_fn(output, Y)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.numpy():.6f}\")\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return time.time() - start_time\n",
        "\n",
        "\n",
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    seq_length = 20\n",
        "    batch_size = 32\n",
        "    input_size = 10\n",
        "    hidden_size = 128\n",
        "    num_epochs = 10\n",
        "\n",
        "    np.random.seed(123)\n",
        "    X_train = np.random.rand(1000, seq_length, input_size).astype(np.float32)\n",
        "    Y_train = X_train.copy()\n",
        "\n",
        "    # TensorFlow\n",
        "    tensorflow_time = benchmark_rnn_models(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "\n",
        "    print(f\"TensorFlow Time: {tensorflow_time:.4f} s\")\n",
        "\n",
        "run_benchmark()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
