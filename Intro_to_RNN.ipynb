{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuInr3Sk0J3C"
      },
      "source": [
        "# Tutorial 3: Intro to Recurrent Neural Networks: Math, Training, and the Copy Task\n",
        "\n",
        "# Instructor: Dr. Ankur Mali\n",
        "# University of South Florida (Spring 2025)\n",
        "### In this tutorial we will build RNNs based on equation and will compare 3 popular frameworks (Jax, TensorFlow and Pytorch)\n",
        "\n",
        "## Vanilla RNN -- For more in depth explanation refer to your slides\n",
        "\n",
        "### Forward Pass (Inference) -- Stage 1\n",
        "Given an input at time \\(t\\):\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}_t \\in \\mathbb{R}^{d_{\\text{in}}},\\quad \\mathbf{h}_{t-1} \\in \\mathbb{R}^{d_{\\text{hid}}}\n",
        "\\end{aligned}\n",
        "we define RNN parameters:\n",
        "\\begin{aligned}\n",
        "\\mathbf{W}_x \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{hid}}}, \\quad\n",
        "\\mathbf{W}_h \\in \\mathbb{R}^{d_{\\text{hid}} \\times d_{\\text{hid}}}, \\quad\n",
        "\\mathbf{b}_h \\in \\mathbb{R}^{d_{\\text{hid}}}.\n",
        "\\end{aligned}\n",
        "\n",
        "The hidden state update:\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_t = \\tanh\\Bigl(\\mathbf{x}_t\\,\\mathbf{W}_x \\;+\\;\\mathbf{h}_{t-1}\\,\\mathbf{W}_h \\;+\\;\\mathbf{b}_h\\Bigr).\n",
        "\\end{aligned}\n",
        "\n",
        "Over a sequence  ($\\mathbf{x}_1$, $\\dots$, $\\mathbf{x}_T$), we unroll:\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_0 = \\mathbf{0},\\quad\n",
        "\\mathbf{h}_1 = \\tanh(\\mathbf{x}_1 \\mathbf{W}_x + \\mathbf{h}_0 \\mathbf{W}_h + \\mathbf{b}_h),\\,\\dots,\\,\n",
        "\\mathbf{h}_T = \\tanh(\\mathbf{x}_T \\mathbf{W}_x + \\mathbf{h}_{T-1} \\mathbf{W}_h + \\mathbf{b}_h).\n",
        "\\end{aligned}\n",
        "\n",
        "Optionally, each hidden state  \\($\\mathbf{h}_t$\\) can be projected to the output dimension $d_{\\text{in}}$:\n",
        "\\begin{aligned}\n",
        "\\mathbf{\\hat{y}}_t = \\mathbf{h}_t \\mathbf{W}_{\\text{out}} + \\mathbf{b}_{\\text{out}}\n",
        "\\end{aligned}\n",
        "\n",
        "<!-- $\\mathbf{\\hat{y}}$_t = $\\mathbf{h}_t$,$\\mathbf{W}_{\\text{out}}$ + $\\mathbf{b}_{\\text{out}}$. -->\n",
        "\n",
        "\n",
        "### Remaining Stages\n",
        "We define a loss (Stage 2) over all time steps, for instance:\n",
        "\\begin{aligned}\n",
        "\\mathbf{L} = \\frac{1}{T} \\sum_{t=1}^T \\left\\|\\,\\mathbf{\\hat{y}}_t - \\mathbf{y}_t\\,\\right\\|^2,\n",
        "\\end{aligned}\n",
        "and use Backpropagation Through Time (BPTT) (Stage 3). An optimizer (e.g., Adam) updates parameters (Stage 4):\n",
        "\\begin{aligned}\n",
        "\\theta \\,\\leftarrow\\, \\theta \\;-\\; \\eta \\,\\nabla_\\theta \\,\\mathbf{L}.\n",
        "\\end{aligned}\n",
        "\n",
        "---\n",
        "\n",
        "## GRU\n",
        "\n",
        "### Forward Pass (Inference)\n",
        "A Gated Recurrent Unit includes reset $\\mathbf{r}_t$ and update $\\mathbf{z}_t$ gates:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_t &= \\sigma\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_z + \\mathbf{h}_{t-1}\\,\\mathbf{U}_z + \\mathbf{b}_z\\bigr), \\\\\n",
        "\\mathbf{r}_t &= \\sigma\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_r + \\mathbf{h}_{t-1}\\,\\mathbf{U}_r + \\mathbf{b}_r\\bigr), \\\\\n",
        "\\tilde{\\mathbf{h}}_t &= \\tanh\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_h + (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1})\\,\\mathbf{U}_h + \\mathbf{b}_h\\bigr), \\\\\n",
        "\\mathbf{h}_t &= (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} \\;+\\; \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t.\n",
        "\\end{aligned}\n",
        "\n",
        "where $\\sigma$ is the sigmoid function, and $\\odot$ denotes elementwise multiplication.\n",
        "\n",
        "### Remaining Stages\n",
        "As in the vanilla RNN, define a loss $\\mathbf{L}$ (e.g. MSE). The same BPTT logic applies, but the derivatives now include the GRU gating operations. Parameters (e.g., $\\mathbf{W}_z, \\mathbf{U}_z, \\ldots$ ) are updated by any gradient-based optimizer.\n",
        "\n",
        "---\n",
        "\n",
        "## Optimizer\n",
        "A typical training loop includes:\n",
        "\n",
        "1. **Forward pass**: compute model outputs $\\mathbf{\\hat{y}}_t$.\n",
        "2. **Loss computation**: $\\mathbf{L}(\\mathbf{\\hat{y}}_t, \\mathbf{y}_t)$.\n",
        "3. **Backward pass**: compute $\\nabla_\\theta \\mathbf{L}$ via BPTT.\n",
        "4. **Parameter update**:\n",
        "   \\begin{aligned}\n",
        "   \\theta \\leftarrow \\theta - \\eta \\;\\nabla_\\theta \\,\\mathcal{L}.\n",
        "   \\end{aligned}\n",
        "   (For example, using Adam, SGD, RMSProp, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "## The Copy Task\n",
        "The **copy task** is a simple sequence-to-sequence challenge:\n",
        "\n",
        "- **Input**: a sequence of random vectors {$\\mathbf{x}_1, \\dots, \\mathbf{x}_T$}.\n",
        "- **Target**: the **same** sequence {$\\mathbf{x}_1, \\dots, \\mathbf{x}_T$}.\n",
        "\n",
        "Thus, the model should learn to produce $\\mathbf{\\hat{y}}_t \\approx \\mathbf{x}_t$ at each time step ($t$). It's a straightforward yet revealing test of a model’s capacity to retain and reproduce a sequence—particularly sensitive to the model’s ability to **remember** information over time.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c6lcugDetLTx"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# Custom RNN Cell (Core Computation)\n",
        "########################################\n",
        "\n",
        "########################################\n",
        "# TensorFlow Implementation\n",
        "########################################\n",
        "\n",
        "# ------- Single-Step RNN Cell -------\n",
        "class RNNCellTF(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in TensorFlow.\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_x = self.add_weight(\n",
        "            shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.W_h = self.add_weight(\n",
        "            shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b_h = self.add_weight(\n",
        "            shape=(hidden_size,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        h_t = tf.math.tanh(\n",
        "            tf.matmul(x_t, self.W_x) + tf.matmul(h_prev, self.W_h) + self.b_h\n",
        "        )\n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class RNNTF(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = RNNCellTF(input_size, hidden_size)\n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(\n",
        "            shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b_out = self.add_weight(\n",
        "            shape=(input_size,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.rnn_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "# Training / Benchmark\n",
        "########################################\n",
        "\n",
        "# -------------- TensorFlow Benchmark --------------\n",
        "def benchmark_tensorflow(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    model = RNNTF(input_size, hidden_size)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    X_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y_tf = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(X_tf)\n",
        "            loss = loss_fn(output, Y_tf)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        #print(f\"Epoch {epoch} | Loss TF: {loss.numpy():.6f}\")\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "\n",
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    seq_length = 20\n",
        "    batch_size = 32\n",
        "    input_size = 10\n",
        "    hidden_size = 128\n",
        "    num_epochs = 10\n",
        "\n",
        "    np.random.seed(42)\n",
        "    X_train = np.random.rand(1000, seq_length, input_size).astype(np.float32)\n",
        "    Y_train = X_train.copy()\n",
        "\n",
        "    # TensorFlow\n",
        "    tensorflow_time = benchmark_tensorflow(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "\n",
        "    print(f\"TensorFlow Time: {tensorflow_time:.4f} s\")\n",
        "\n",
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Standard LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StandardLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input gate weights\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        i_t = tf.sigmoid(tf.matmul(x_t, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_t, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_t, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_t, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class StandardLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = StandardLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiplicative LSTM RNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiplicativeLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Learnable memory weights and bias\n",
        "        self.W_m = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_m = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_m = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        # Input gate weights and bias\n",
        "        self.W_i = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Forget gate weights and bias\n",
        "        self.W_f = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Output gate weights and bias\n",
        "        self.W_o = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "        \n",
        "        # Cell candidate weights and bias\n",
        "        self.W_c = self.add_weight(shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(hidden_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, x_t, h_prev, c_prev):\n",
        "        # Multiplicative Extension\n",
        "        m_t = tf.matmul(x_t, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        x_cap = m_t * x_t\n",
        "\n",
        "        i_t = tf.sigmoid(tf.matmul(x_cap, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        f_t = tf.sigmoid(tf.matmul(x_cap, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        o_t = tf.sigmoid(tf.matmul(x_cap, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        c_hat = tf.tanh(tf.matmul(x_cap, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        \n",
        "        c_t = f_t * c_prev + i_t * c_hat\n",
        "        h_t = o_t * tf.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "\n",
        "class MultiplicativeLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = MultiplicativeLSTMCell(input_size, hidden_size)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True)\n",
        "        self.b_out = self.add_weight(shape=(input_size,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        c = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "\n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h, c = self.lstm_cell(x_t, h, c)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnUjvLTtUys",
        "outputId": "0d895fad-60ab-4637-cf95-2df4bc18466c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.330919\n",
            "Epoch 1 | Loss: 0.209169\n",
            "Epoch 2 | Loss: 0.179643\n",
            "Epoch 3 | Loss: 0.120039\n",
            "Epoch 4 | Loss: 0.136909\n",
            "Epoch 5 | Loss: 0.121190\n",
            "Epoch 6 | Loss: 0.097766\n",
            "Epoch 7 | Loss: 0.090170\n",
            "Epoch 8 | Loss: 0.098913\n",
            "Epoch 9 | Loss: 0.092541\n",
            "TensorFlow Time: 5.7567 s\n"
          ]
        }
      ],
      "source": [
        "def benchmark_rnn_models(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    model = RNNTF(input_size, hidden_size)\n",
        "\n",
        "    X = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "\n",
        "    # Standard LSTM Benchmark\n",
        "    model = StandardLSTM(input_size, hidden_size)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(X)\n",
        "            loss = loss_fn(output, Y)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.numpy():.6f}\")\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return time.time() - start_time\n",
        "\n",
        "\n",
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    seq_length = 20\n",
        "    batch_size = 32\n",
        "    input_size = 10\n",
        "    hidden_size = 128\n",
        "    num_epochs = 10\n",
        "\n",
        "    np.random.seed(123)\n",
        "    X_train = np.random.rand(1000, seq_length, input_size).astype(np.float32)\n",
        "    Y_train = X_train.copy()\n",
        "\n",
        "    # TensorFlow\n",
        "    tensorflow_time = benchmark_rnn_models(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "\n",
        "    print(f\"TensorFlow Time: {tensorflow_time:.4f} s\")\n",
        "\n",
        "run_benchmark()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
